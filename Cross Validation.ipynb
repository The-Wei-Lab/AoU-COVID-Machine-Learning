{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements Here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "rng = np.random.default_rng()\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "#from venn import venn\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import os\n",
    "import subprocess\n",
    "import shap\n",
    "cutOff=0.90\n",
    "\n",
    "target = 'long_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import other Functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeInBucket(name_of_file_loc, a = 'dataFrame'):\n",
    "    # get the bucket name\n",
    "    \n",
    "    # This snippet assumes you run setup first\n",
    "\n",
    "    # This code saves your dataframe into a csv file in a \"data\" folder in Google Bucket\n",
    "\n",
    "    # Replace df with THE NAME OF YOUR DATAFRAME\n",
    "    if (type(name_of_file_loc)== type('string')):\n",
    "        my_dataframe = pd.read_csv(name_of_file_loc)\n",
    "        destination_filename = name_of_file_loc\n",
    "    else:\n",
    "        my_dataframe = name_of_file_loc\n",
    "        destination_filename = a\n",
    "\n",
    "    # Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
    "    \n",
    "\n",
    "    ########################################################################\n",
    "    ##\n",
    "    ################# DON'T CHANGE FROM HERE ###############################\n",
    "    ##\n",
    "    ########################################################################\n",
    "\n",
    "    # save dataframe in a csv file in the same workspace as the notebook\n",
    "    my_dataframe.to_csv(destination_filename, index=False)\n",
    "\n",
    "    # get the bucket name\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file to the bucket\n",
    "    args = [\"gsutil\", \"cp\", f\"./{destination_filename}\", f\"{my_bucket}/data/\"]\n",
    "    output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "    # print output from gsutil\n",
    "    output.stderr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFromBucket(name_of_file_in_bucket):\n",
    "    # get the bucket name\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file from the bucket to the current working space\n",
    "    os.system(f\"gsutil cp '{my_bucket}/data/{name_of_file_in_bucket}' .\")\n",
    "\n",
    "    print(f'[INFO] {name_of_file_in_bucket} is successfully downloaded into your working space')\n",
    "    # save dataframe in a csv file in the same workspace as the notebook\n",
    "    my_dataframe = pd.read_csv(name_of_file_in_bucket)\n",
    "    return my_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a bunch of dataframes, and joins them by the joining variable\n",
    "    #duplicate columns will be dropped at the end, \n",
    "    #so if there are still duplicate column names, that means some of the columns don't match\n",
    "def combineData(dataFrameList, joinVariable):\n",
    "    joiningSet = dataFrameList[0].loc[:,joinVariable]\n",
    "    \n",
    "    \n",
    "    #Now redefine joiningSet by going through each dataframe in the list \n",
    "    #and redefine as only the joiningVariables that are both in that dataframe, and the previous joiningSet\n",
    "    for df in dataFrameList:\n",
    "        joiningSet = df.loc[df[joinVariable].isin(joiningSet),joinVariable]\n",
    "    \n",
    "    \n",
    "    #now that we have a joiningSet that includes only the common part of every data set, \n",
    "    #we need to restrict each data frame according to that set\n",
    "    \n",
    "    cleanedDfList = []\n",
    "    for df in dataFrameList:\n",
    "        cleanedDf = df.loc[df[joinVariable].isin(joiningSet),:].sort_values(joinVariable, ignore_index=True)\n",
    "        cleanedDfList = cleanedDfList + [cleanedDf]\n",
    "        \n",
    "    #now join them all together with a concat statement and remove duplicate columns\n",
    "    returnDf=pd.concat(cleanedDfList,axis=1)\n",
    "    returnDf = returnDf.T.drop_duplicates().T\n",
    "    return returnDf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the simplifying assumption that takes the data and changes the long_covid valuies of high y_pred patients\n",
    "\n",
    "#machineLearningDf\n",
    "def simplifyingAssumption(df, p=0.9):\n",
    "    for i in range(0,len(df)):\n",
    "        if (df.loc[i,'y_pred'] >= p):\n",
    "            df.loc[i,'long_covid']=1\n",
    "        \n",
    "    return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonNumeric(df):\n",
    "    df = df[df['long_covid'] > - 1]\n",
    "    df = df.replace('NaN', np.NaN)\n",
    "    df = df.replace('nan', np.NaN)\n",
    "    for a in df.columns:\n",
    "    #columnDf = trialDf[a]\n",
    "        columnMean = df.loc[df[a].notna() ,a].mean()\n",
    "    #print(a)\n",
    "    #print(columnMean)\n",
    "        df[a] = df[a].fillna(value = columnMean)\n",
    "    returnDf = df\n",
    "    return returnDf    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAUROCValues (predValues, testy, modelLabel):\n",
    "    \n",
    "    lr_probs = predValues\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #lr_probs = lr_probs[:, 1]\n",
    "    \n",
    "    listName = []\n",
    "    # calculate scores\n",
    "    lr_auc = roc_auc_score(testy, lr_probs)\n",
    "    \n",
    "    # summarize scores\n",
    "    print(modelLabel+': ROC AUC=%.3f' % (lr_auc))\n",
    "    \n",
    "    # calculate roc curves\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "    returnList = roc_curve(testy, lr_probs)\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def createAUROC (predValues, testy, modelLabel):\n",
    "    \n",
    "    ## generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(testy))]\n",
    "    \n",
    "    lr_probs = predValues\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #lr_probs = lr_probs[:, 1]\n",
    "    \n",
    "    # calculate scores\n",
    "    #ns_auc = roc_auc_score(testy, ns_probs)\n",
    "    lr_auc = roc_auc_score(testy, lr_probs)\n",
    "    \n",
    "    ## summarize scores\n",
    "    #print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    #print(modelLabel+': ROC AUC=%.3f' % (lr_auc))\n",
    "    \n",
    "    ## calculate roc curves\n",
    "    #ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "    #lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "    \n",
    "    ## plot the roc curve for the model\n",
    "    #pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    #pyplot.plot(lr_fpr, lr_tpr, marker='.', label=modelLabel)\n",
    "    \n",
    "    ## axis labels\n",
    "    #pyplot.xlabel('1 - Specificity')\n",
    "    #pyplot.ylabel('Sensitivity')\n",
    "    \n",
    "    ## show the legend\n",
    "    #pyplot.legend()\n",
    "    \n",
    "    ## show the plot\n",
    "    #pyplot.show()\n",
    "    \n",
    "    return lr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(train, test, predictorVars,targetVar, model):\n",
    "    x=train.loc[:,predictorVars]\n",
    "    y=train[targetVar]\n",
    "    xTest=test.loc[:,predictorVars]\n",
    "    yTest=test[targetVar]\n",
    "\n",
    "    regressor = model\n",
    "    regressor.fit(x, y) \n",
    "    \n",
    "    print(\"R-Squared Train:\",metrics.r2_score(y, regressor.predict(x)))\n",
    "    print(\"R-Squared Test:\",metrics.r2_score(yTest, regressor.predict(xTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePredValues(train, test, model, predVars):\n",
    "    \n",
    "    regressor = model\n",
    "    regressor.fit(train.loc[:,predVars], train[target])\n",
    "    averageValues = []\n",
    "    nonRounded = []\n",
    "    predictions = regressor.predict(test.loc[:,predVars])\n",
    "    #print(len(testDf))\n",
    "    \n",
    "    for i in range(0,len(testDf)):\n",
    "        modelPred = predictions[i]\n",
    "        \n",
    "        if float(modelPred) <=0:\n",
    "            modelPred = 0\n",
    "        if float(modelPred) >=1:\n",
    "            modelPred = 1\n",
    "        roundNum  = round(modelPred,0)\n",
    "        averageValues = averageValues +[roundNum]\n",
    "        nonRounded = nonRounded +[modelPred]\n",
    "        \n",
    "    predValues=averageValues\n",
    "        \n",
    "    return predValues, nonRounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    list type, with optimal cutoff value\n",
    "        \n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    \n",
    "    #Return \"threshold where sensitivity ~0.70\"\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'sensitivity' : pd.Series(tpr, index=i), 'quality_score' : pd.Series(abs(0.7-tpr), index=i),\n",
    "                        'specificity' : pd.Series((1.-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc = roc.sort_values('quality_score', ascending = True)\n",
    "    roc = roc.reset_index(drop=True)\n",
    "    #print(roc.head(1))\n",
    "    roc_t = roc.loc[0,['threshold']]\n",
    "\n",
    "    return list(roc_t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedAverage(trainDf, testDf, model,predictorVariables, p):\n",
    "    q = 1-p\n",
    "    regressor = model\n",
    "    regressor.fit(trainDf.loc[:,predictorVariables], trainDf[target])\n",
    "    averageValues = []\n",
    "    nonRounded = []\n",
    "    predictions = regressor.predict(testDf.loc[:,predictorVariables])\n",
    "    #print(len(testDf))\n",
    "    \n",
    "    for i in range(0,len(testDf)):\n",
    "        modelPred = predictions[i]\n",
    "        weightedAverage = (q*(modelPred)+p*(testDf.loc[i,'y_pred']))\n",
    "        \n",
    "        if float(weightedAverage) <= 0:\n",
    "            weightedAverage = 0\n",
    "        if float(weightedAverage) >=1:\n",
    "            weightedAverage = 1\n",
    "        \n",
    "        \n",
    "        nonRounded = nonRounded + [weightedAverage]\n",
    "        \n",
    "    cuttOff = Find_Optimal_Cutoff(testDf[target], pd.Series(nonRounded))\n",
    "    \n",
    "    for pred_prob in nonRounded:\n",
    "        roundNum = applyCutOff(pred_prob, cuttOff[0])\n",
    "        averageValues = averageValues +[roundNum]\n",
    "    \n",
    "    predValues=averageValues\n",
    "        \n",
    "    return predValues, nonRounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyCutOff(number, a):\n",
    "    if number >= a:\n",
    "        returnNumber = 1\n",
    "    if number < a:\n",
    "        returnNumber = 0\n",
    "        \n",
    "    return returnNumber "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triWeightedAverage(trainDf, testDf, model1, model2,genePredictorVariables, surveyPredictorVariables, p,q):\n",
    "    x = 1-(p+q)\n",
    "    regressor1 = model1\n",
    "    regressor1.fit(trainDf.loc[:,genePredictorVariables], trainDf[target])\n",
    "    \n",
    "    regressor2 = model2\n",
    "    regressor2.fit(trainDf.loc[:,surveyPredictorVariables], trainDf[target])\n",
    "    \n",
    "    averageValues = []\n",
    "    nonRounded = []\n",
    "    genePredictions = regressor1.predict(testDf.loc[:,genePredictorVariables])\n",
    "    surveyPredictions = regressor2.predict(testDf.loc[:,surveyPredictorVariables])\n",
    "    #print(len(testDf))\n",
    "    \n",
    "    for i in range(0,len(testDf)):\n",
    "        genePred = genePredictions[i]\n",
    "        surveyPred = surveyPredictions[i]\n",
    "        weightedAverage = (p*(genePred)+q*(surveyPred)+x*(testDf.loc[i,'y_pred']))\n",
    "        \n",
    "        #To change the cut off, we have to change these statements possible\n",
    "  \n",
    "        if float(weightedAverage) <= 0:\n",
    "            weightedAverage = 0\n",
    "        if float(weightedAverage) >=1:\n",
    "            weightedAverage = 1\n",
    "            \n",
    "        nonRounded = nonRounded + [weightedAverage]\n",
    "        \n",
    "    cuttOff = Find_Optimal_Cutoff(testDf[target], pd.Series(nonRounded))\n",
    "    \n",
    "    for pred_prob in nonRounded:\n",
    "        roundNum = applyCutOff(pred_prob, cuttOff[0])\n",
    "        averageValues = averageValues +[roundNum]\n",
    "        \n",
    "    return predValues, nonRounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title = 'Confusion Matrix of EHR and Genetic Data Classifier'\n",
    "\n",
    "def generateConfusionMatrix(test, predValues, title):\n",
    "    confusionMatrix = metrics.confusion_matrix(test['long_covid'], predValues)\n",
    "    \n",
    "    tp = confusionMatrix[1,1] \n",
    "    tn = confusionMatrix[0,0]\n",
    "    fp = confusionMatrix[0,1]\n",
    "    fn = confusionMatrix[1,0] \n",
    "    \n",
    "    sensitivity = tp / (tp+ fn)\n",
    "    specificity = tn / (fp + tn)\n",
    "    PPV = tp / (tp+ fp)\n",
    "    \n",
    "    \n",
    "    return sensitivity, specificity, PPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTestAndTrainSets(df, trainProp):\n",
    "    trainDf = df[df['long_covid']==0].sample(frac=trainProp, random_state=101)\n",
    "    trainDf =pd.concat([trainDf,df[df['long_covid']==1].sample(frac=trainProp, random_state=101)], ignore_index=True)\n",
    "    \n",
    "    NotInList=[]\n",
    "    \n",
    "    testDf =  df\n",
    "\n",
    "\n",
    "    toDropList = df.index[df['person_id'].isin(trainDf['person_id'].tolist() + NotInList)]\n",
    "\n",
    "    #print(toDropList)\n",
    "    testDf = testDf.drop(index = toDropList, axis=0)\n",
    "    testDf = testDf.reset_index(drop=True)\n",
    "    return trainDf, testDf\n",
    "    #trainDf=trainDf.drop(['min_long_covid_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortById(df):\n",
    "    returnDf = df.sort_values('person_id')\n",
    "    returnDf = returnDf.reset_index(drop=True)\n",
    "    return returnDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new functions to iterate through using loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to break a dataframe into n smaller dataframes of equal number of rows\n",
    "\n",
    "def splitDataFrame(df, n):\n",
    "    dfList = []\n",
    "    denomonator = n\n",
    "    \n",
    "    df = removeNonNumeric(df)\n",
    "    \n",
    "    #Make a recursive loop, that creates a dataframe based on 1/n of the remaining data\n",
    "    #Then, increment n=n-1 until you created all of the random sets\n",
    "    for a in range (0,n):\n",
    "        if denomonator <= 1:\n",
    "            nextDf = df\n",
    "        else:\n",
    "            proportionSplit = 1.0 / denomonator\n",
    "            nextDf, remainingDf = makeTestAndTrainSets(df, proportionSplit)\n",
    "        dfList = dfList + [nextDf]\n",
    "        denomonator = denomonator - 1\n",
    "        df = remainingDf\n",
    "    \n",
    "    return dfList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to select all but one elements of a list of dataframes and create a test a training set\n",
    "\n",
    "def selectTestAndTrain(dfList, index, n):\n",
    "    \n",
    "    indexList = list(range(0,n))\n",
    "    testSet = dfList[index]\n",
    "    #print(indexList)\n",
    "    indexList.remove(index)\n",
    "    #print(indexList)\n",
    "    trainingSet = pd.concat(list(pd.Series(dfList)[indexList]))\n",
    "    \n",
    "    return testSet, trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function that trains the Genetic Data and returns a ROC, Specificity, and Sensitivity \n",
    "#(sensitivity and specificity @ .5 cut off )\n",
    "\n",
    "#Modify to return the model as well\n",
    "\n",
    "def TrainGeneticData(geneTest, geneTrain, genePredVars):\n",
    "    #geneTrain = geneTrain.drop('person_id', axis=1)\n",
    "    #geneTest = geneTest.drop('person_id', axis=1)\n",
    "    modelType =  MLPRegressor(random_state=0, n_iter_no_change = 50, \n",
    "                                              hidden_layer_sizes = (550,)\n",
    "                                             ,max_iter = 500)\n",
    "    predValues, nonRoundedPredValues = weightedAverage(geneTrain, geneTest, \n",
    "                                modelType,\n",
    "                                genePredVars, 0.79)\n",
    "    \n",
    "    sensitivity, specificity, ppv = generateConfusionMatrix(geneTest, predValues, \" \")\n",
    "    \n",
    "    ROC = createAUROC(nonRoundedPredValues, geneTest['long_covid'], \" \")\n",
    "    \n",
    "    resultsList = [sensitivity, specificity, ROC, ppv]\n",
    "    \n",
    "    model = KNeighborsRegressor(n_neighbors=3)\n",
    "    model.fit(geneTrain.loc[:,genePredVars], geneTrain['long_covid'])\n",
    "    \n",
    "    return resultsList, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify to return the model as well\n",
    "def TrainSurveyData(surveyTest, surveyTrain):\n",
    "    \n",
    "    surveyTrain = surveyTrain.drop('person_id', axis=1)\n",
    "    surveyTest = surveyTest.drop('person_id', axis=1)\n",
    "    \n",
    "    withNaTest = surveyTest\n",
    "    withNaTrain = surveyTrain.drop('y_pred', axis=1)\n",
    "    \n",
    "    yTrain = withNaTrain['long_covid']\n",
    "    yTest = withNaTest['long_covid']\n",
    "\n",
    "    xTrain = withNaTrain.loc[:, withNaTrain.columns !='long_covid']\n",
    "    xTest = withNaTest.loc[:, withNaTest.columns != 'long_covid']\n",
    "\n",
    "    xTrain = xTrain.loc[:, xTrain.columns !='y_pred']\n",
    "    xTest = xTest.loc[:, xTest.columns != 'y_pred']\n",
    "    #xTest = xTest.loc[:, xTest.columns != 'person_id']\n",
    "    \n",
    "    train = xgb.DMatrix(xTrain, label = yTrain)\n",
    "    test = xgb.DMatrix(xTest, label = yTest)\n",
    "    param = {'max_dpeth': 5,\n",
    "            'eta':0.6,\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class':3}\n",
    "    epochs = 250\n",
    "    \n",
    "    model=xgb.train(param, train, epochs)\n",
    "    predXGB = model.predict(test)\n",
    "    predValues = []\n",
    "    cuttOff = Find_Optimal_Cutoff(withNaTest['long_covid'], predXGB)\n",
    "    averageValues = []\n",
    "    for pred_prob in predXGB:\n",
    "        roundNum = applyCutOff(pred_prob, cuttOff[0])\n",
    "        averageValues = averageValues +[roundNum]\n",
    "    \n",
    "    sensitivity, specificity, ppv = generateConfusionMatrix(withNaTest, averageValues, 'XGBoost Model Using Mobile Device and Survey Data')\n",
    "    \n",
    "    ROC = createAUROC(predXGB, withNaTest['long_covid'], ' ')\n",
    "    \n",
    "    resultsList = [sensitivity, specificity, ROC, ppv]\n",
    "    \n",
    "    return resultsList, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightedPredValues(p, q, geneVars, surveyVars, regressorGene, regressorXGB, goldStandardDf):\n",
    "    #The code below creates it's own gold standard test set\n",
    "    #We created what can function as a gold standard test set using XGBtestSet, so let's use that instead\n",
    "    #clean up that test set\n",
    "    goldStandardDf = goldStandardDf.sort_values('person_id')\n",
    "    \n",
    "    goldStandardDf = goldStandardDf.reset_index(drop=True)\n",
    "\n",
    "    withNaTest = goldStandardDf[surveyVars].drop('y_pred', axis=1)\n",
    "    withNaTest = withNaTest.drop('person_id', axis=1)\n",
    "    \n",
    "    yTest = goldStandardDf['long_covid']\n",
    "    #print(len(yTest))\n",
    "    xTest = withNaTest.loc[:, withNaTest.columns != 'long_covid']\n",
    "    #xTest = xTest.loc[:, xTest.columns != 'y_pred']\n",
    "    test = xgb.DMatrix(xTest, label = yTest)\n",
    "    \n",
    "    geneTestDf = goldStandardDf[genePredVars]\n",
    "\n",
    "    XGBTestDf = withNaTest\n",
    "\n",
    "    x = 1-(p+q)\n",
    "\n",
    "    averageValues = []\n",
    "    genePredictions = regressorGene.predict(geneTestDf)\n",
    "    surveyPredictions =  regressorXGB.predict(test)\n",
    "    #print(len(testDf))\n",
    "    \n",
    "    for i in range(0,len(geneTestDf)):\n",
    "        genePred = genePredictions[i]\n",
    "        surveyPred = surveyPredictions[i]\n",
    "        weightedAverage = (p*(genePred)+q*(surveyPred)+x*(goldStandardDf.loc[i,'y_pred']))\n",
    "        roundNum = weightedAverage\n",
    "        if float(roundNum) <= 0:\n",
    "            roundNum = 0\n",
    "        if float(roundNum) >=1:\n",
    "            roundNum = 1\n",
    "        averageValues = averageValues +[roundNum]\n",
    "    \n",
    "    predValuesNotRounded=averageValues\n",
    "    return predValuesNotRounded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to apply a full evaluation method to our goldStandard cohort\n",
    "\n",
    "def goldStandardEvaluation(geneVars, surveyVars, regressorGene, regressorXGB, goldStandardDf):\n",
    "    \n",
    "    #The code below creates it's own gold standard test set\n",
    "    #We created what can function as a gold standard test set using XGBtestSet, so let's use that instead\n",
    "    #clean up that test set\n",
    "    goldStandardDf = goldStandardDf.sort_values('person_id')\n",
    "    \n",
    "    goldStandardDf = goldStandardDf.reset_index(drop=True)\n",
    "\n",
    "    withNaTest = goldStandardDf[surveyVars].drop('y_pred', axis=1)\n",
    "    withNaTest = withNaTest.drop('person_id', axis=1)\n",
    "    \n",
    "    yTest = goldStandardDf['long_covid']\n",
    "    #print(len(yTest))\n",
    "    xTest = withNaTest.loc[:, withNaTest.columns != 'long_covid']\n",
    "    #xTest = xTest.loc[:, xTest.columns != 'y_pred']\n",
    "    test = xgb.DMatrix(xTest, label = yTest)\n",
    "    \n",
    "    geneTestDf = goldStandardDf[genePredVars]\n",
    "\n",
    "    XGBTestDf = withNaTest\n",
    "    p = 0.05 #/ 0.63\n",
    "    #q = 0.00\n",
    "    #p = 0\n",
    "    q = 0.37 #/ 0.95\n",
    "\n",
    "    x = 1-(p+q)\n",
    "\n",
    "    averageValues = []\n",
    "    genePredictions = regressorGene.predict(geneTestDf)\n",
    "    surveyPredictions =  regressorXGB.predict(test)\n",
    "    #print(len(testDf))\n",
    "    nonRounded = []\n",
    "    for i in range(0,len(geneTestDf)):\n",
    "        genePred = genePredictions[i]\n",
    "        surveyPred = surveyPredictions[i]\n",
    "        weightedAverage = (p*(genePred)+q*(surveyPred)+x*(goldStandardDf.loc[i,'y_pred']))\n",
    "        if float(weightedAverage) <= 0:\n",
    "            weightedAverage = 0\n",
    "        if float(weightedAverage) >=1:\n",
    "            weightedAverage = 1\n",
    "        nonRounded = nonRounded + [weightedAverage]\n",
    "        \n",
    "    cutOff = Find_Optimal_Cutoff(goldStandardDf['long_covid'], pd.Series(nonRounded))\n",
    "    #print(cutOff)\n",
    "    averageValues = []\n",
    "    for pred_prob in nonRounded:\n",
    "        roundNum = applyCutOff(pred_prob, cutOff[0])\n",
    "        averageValues = averageValues +[roundNum]\n",
    "    predValues = averageValues\n",
    "    predValuesNotRounded = nonRounded\n",
    "        \n",
    "    sensitivity, specificity, ppv = generateConfusionMatrix(goldStandardDf, predValues, ' ')\n",
    "    \n",
    "    ROC = createAUROC(predValuesNotRounded, goldStandardDf['long_covid'], ' ')\n",
    "    \n",
    "    EHRroc = createAUROC(goldStandardDf['y_pred'], goldStandardDf['long_covid'], ' ')\n",
    "    \n",
    "    surveyShap = getShap(regressorXGB, test)\n",
    "    #geneShap = getShap(regressorGene, geneTestDf, modelType = 'NN' )    \n",
    "    \n",
    "    shapAppend = pd.DataFrame.from_dict({'Survey_Shap_Values': [surveyShap]\n",
    "                  #, 'Genetic_Shap_Values': geneShap\n",
    "                 })\n",
    "    \n",
    "    #sens,spec,roc, ppv, EHRroc, shapAppend\n",
    "        \n",
    "    return sensitivity, specificity, ROC, ppv, EHRroc, shapAppend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to return shap values for a model\n",
    "def getShap(model, testDf, modelType = 'XGB'):\n",
    "    \n",
    "    if modelType == 'XGB':\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "    if modelType == 'KNN':\n",
    "        explainer = shap.KernelExplainer(model.predict, testDf)\n",
    "        \n",
    "    shap_values = explainer.shap_values(testDf)\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set paramenters for the function below, and define internal functions\n",
    "\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'size'   : 16}\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "class AnyObject(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class data_handler(object):\n",
    "    def legend_artist(self, legend, orig_handle, fontsize, handlebox):\n",
    "        scale = fontsize / 12\n",
    "        x0, y0 = handlebox.xdescent, handlebox.ydescent\n",
    "        width, height = handlebox.width, handlebox.height\n",
    "        patch_sq1 = mpatches.Rectangle([x0, y0 + height/2 * (1 - scale) ], height * scale, height * scale, alpha=0.3, \n",
    "                                      facecolor='darkorange',transform=handlebox.get_transform())\n",
    "        patch_sq2 = mpatches.Rectangle([x0 + width/2 ,y0 + height/2 * (1 - scale) ], height * scale, height * scale, alpha=0.3, \n",
    "                                        facecolor='lightblue',transform=handlebox.get_transform())\n",
    "        patch_sq3 = mpatches.Rectangle([x0 + width ,y0 + height/2 * (1 - scale) ], height * scale, height * scale, alpha=0.3, \n",
    "                                        facecolor='limegreen',transform=handlebox.get_transform())\n",
    "        patch_sq4 = mpatches.Rectangle([x0 + width*(3/2) ,y0 + height/2 * (1 - scale) ], height * scale, height * scale, alpha=0.3, \n",
    "                                        facecolor='lightpink',transform=handlebox.get_transform())\n",
    "\n",
    "\n",
    "        handlebox.add_artist(patch_sq1)\n",
    "        handlebox.add_artist(patch_sq2)\n",
    "        handlebox.add_artist(patch_sq3)\n",
    "        handlebox.add_artist(patch_sq4)\n",
    "\n",
    "        return patch_sq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to make a weighted AUROC curve with different Iterations\n",
    "\n",
    "#We start by copy and pasting Chou's code:\n",
    "\n",
    "def curveWeighted(gene_model_list, survey_model_list, gold_standard_list, geneVars, surveyVars,k):\n",
    "    #Parameters\n",
    "        #gene_model_list: the list of genetic Models Used\n",
    "        #survey_model_list: The list of survey models used\n",
    "        #gold_standard_list: The list of Gold Standard Test sets used\n",
    "    ROC_score_list = []\n",
    "    num_of_runs = len(survey_model_list)\n",
    "    \n",
    "    base_fpr = np.linspace(0, 1, 51)\n",
    "    base_recall = np.linspace(0, 1, 51)\n",
    "\n",
    "    whole_A_tpr = []\n",
    "    whole_B_tpr = []\n",
    "    whole_C_tpr = []\n",
    "    whole_D_tpr = []\n",
    "\n",
    "    whole_A_precision = []\n",
    "    whole_B_precision = []\n",
    "    whole_C_precision = []\n",
    "    whole_D_precision = []\n",
    "    # Chris, note that this is only for demonstration purposes of the figure plot logic, so it should be customized to your prediction setting.\n",
    "    for RUN_survey in range(0,num_of_runs):\n",
    "        #Create another loop if we want to also loop through the GS cohort\n",
    "        for RUN_GS in range(0,len(gold_standard_list)):\n",
    "        #RUN_GS = 0\n",
    "            notNeededDf, GoldStandardSelect = selectTestAndTrain(gold_standard_list,RUN_GS,k)\n",
    "            GoldStandardSelect = sortById(GoldStandardSelect)\n",
    "            for RUN_gene in range(0,num_of_runs): # you can understand num_of_runs as number of iterations you want to test. \n",
    "                #Here ineach RUN, I used the same test data, which might not align with your setting.\n",
    "                #Chris: To replace with different versions of test data, use the Gold Standard list\n",
    "\n",
    "                # assume you have 4 models to consider.\n",
    "                # Models: EHR Only, EHR + Genetic, EHR+ Survey, All data\n",
    "\n",
    "                # model_A: EHR ONLY\n",
    "                #y_scores_A = model_A.predict_proba(test_data)  # return two probabilities for each data instance for example [[0.1, 0.9], [0.2,0.8], ...]\n",
    "                #y_scores_A = y_scores_A[:,1]  # only extract the probability of being CASES \n",
    "\n",
    "                #Is this test label?\n",
    "                test_label = GoldStandardSelect['long_covid']\n",
    "\n",
    "                y_scores_A = getWeightedPredValues(0, 0, geneVars, surveyVars, gene_model_list[RUN_gene], survey_model_list[RUN_survey], GoldStandardSelect)\n",
    "                #Replace the lines above with the outcome of a set of models. You can call a function for this\n",
    "\n",
    "                fpr_A, tpr_A, _ = roc_curve(test_label, y_scores_A)\n",
    "                tpr_A = np.interp(base_fpr, fpr_A, tpr_A)\n",
    "                tpr_A[0] = 0.0\n",
    "                #whole_A_tpr = \n",
    "                whole_A_tpr.append(tpr_A)\n",
    "\n",
    "                precision_A, recall_A, _ = skl.metrics.precision_recall_curve(abs(test_label), y_scores_A)\n",
    "                precision_A = np.interp(base_recall, precision_A, recall_A)\n",
    "                precision_A[0] = 1.0\n",
    "                #whole_A_precision= \n",
    "                whole_A_precision.append(precision_A)\n",
    "\n",
    "                # model_B: EHR + Genetic\n",
    "                #y_scores_B = model_B.predict_proba(test_data)\n",
    "                #y_scores_B = y_scores_B[:,1]\n",
    "\n",
    "                y_scores_B = getWeightedPredValues(0.05/0.63, 0, geneVars, surveyVars, gene_model_list[RUN_gene], survey_model_list[RUN_survey], GoldStandardSelect)\n",
    "\n",
    "                fpr_B, tpr_B, _ = roc_curve(test_label, y_scores_B)\n",
    "                tpr_B = np.interp(base_fpr, fpr_B, tpr_B)\n",
    "                tpr_B[0] = 0.0\n",
    "                #whole_B_tpr = \n",
    "                whole_B_tpr.append(tpr_B)\n",
    "\n",
    "                precision_B, recall_B, _ = skl.metrics.precision_recall_curve(abs(test_label), y_scores_B)\n",
    "                precision_B = np.interp(base_recall, precision_B, recall_B)\n",
    "                precision_B[0] = 1.0\n",
    "                #whole_B_precision = \n",
    "                whole_B_precision.append(precision_B)\n",
    "\n",
    "                # model_C: EHR + Survey\n",
    "                #y_scores_C = model_C.predict_proba(test_data)\n",
    "                #y_scores_C = y_scores_C[:,1]\n",
    "\n",
    "                y_scores_C = getWeightedPredValues(0, 0.37/0.95, geneVars, surveyVars, gene_model_list[RUN_gene], survey_model_list[RUN_survey], GoldStandardSelect)\n",
    "\n",
    "                fpr_C, tpr_C, _ = roc_curve(test_label, y_scores_C)\n",
    "                tpr_C = np.interp(base_fpr, fpr_C, tpr_C)\n",
    "                tpr_C[0] = 0.0\n",
    "                #whole_C_tpr=  \n",
    "                whole_C_tpr.append(tpr_C)\n",
    "\n",
    "                precision_C, recall_C, _ = skl.metrics.precision_recall_curve(abs(test_label), y_scores_C)\n",
    "                precision_C = np.interp(base_recall, precision_C, recall_C)\n",
    "                precision_C[0] = 1.0\n",
    "                #whole_C_precision = \n",
    "                whole_C_precision.append(precision_C)\n",
    "\n",
    "                # model_D: All Data Sources\n",
    "                #y_scores_D = model_D.predict_proba(test_data)\n",
    "                #y_scores_D = y_scores_D[:,1]\n",
    "\n",
    "                y_scores_D =getWeightedPredValues(0.05, 0.37, geneVars, surveyVars, gene_model_list[RUN_gene], survey_model_list[RUN_survey], GoldStandardSelect)\n",
    "\n",
    "                fpr_D, tpr_D, _ = roc_curve(test_label, y_scores_D)\n",
    "                tpr_D = np.interp(base_fpr, fpr_D, tpr_D)\n",
    "                tpr_D[0] = 0.0\n",
    "                #whole_D_tpr =  \n",
    "                whole_D_tpr.append(tpr_D)\n",
    "\n",
    "                precision_D, recall_D, _ = skl.metrics.precision_recall_curve(abs(test_label), y_scores_D)\n",
    "                precision_D = np.interp(base_recall, precision_D, recall_D)\n",
    "                precision_D[0] = 1.0\n",
    "                #whole_D_precision = \n",
    "                whole_D_precision.append(precision_D)\n",
    "                \n",
    "                #Add a section of ROC scores to make sure \n",
    "                ROC_score = createAUROC(y_scores_D, GoldStandardSelect['long_covid'], ' ')\n",
    "                ROC_score_list = ROC_score_list + [ROC_score]\n",
    "    print('ROC score average = ' + str(np.array(ROC_score_list).mean()))\n",
    "\n",
    "    whole_A_tpr = np.array(whole_A_tpr)\n",
    "    mean_whole_A_tpr = whole_A_tpr.mean(axis=0)\n",
    "    #print(roc_auc_score(testy, lr_probs))\n",
    "    std_whole_A_tpr = whole_A_tpr.std(axis=0)\n",
    "    whole_A_tprs_upper = np.minimum(mean_whole_A_tpr + std_whole_A_tpr, 1)\n",
    "    whole_A_tprs_lower = mean_whole_A_tpr - std_whole_A_tpr\n",
    "\n",
    "    whole_B_tpr = np.array(whole_B_tpr)\n",
    "    mean_whole_B_tpr = whole_B_tpr.mean(axis=0)\n",
    "    std_whole_B_tpr = whole_B_tpr.std(axis=0)\n",
    "    whole_B_tprs_upper = np.minimum(mean_whole_B_tpr + std_whole_B_tpr, 1)\n",
    "    whole_B_tprs_lower = mean_whole_B_tpr - std_whole_B_tpr\n",
    "\n",
    "    whole_C_tpr = np.array(whole_C_tpr)\n",
    "    mean_whole_C_tpr = whole_C_tpr.mean(axis=0)\n",
    "    std_whole_C_tpr = whole_C_tpr.std(axis=0)\n",
    "    whole_C_tprs_upper = np.minimum(mean_whole_C_tpr + std_whole_C_tpr, 1)\n",
    "    whole_C_tprs_lower = mean_whole_C_tpr - std_whole_C_tpr\n",
    "\n",
    "    whole_D_tpr = np.array(whole_D_tpr)\n",
    "    mean_whole_D_tpr = whole_D_tpr.mean(axis=0)\n",
    "    std_whole_D_tpr = whole_D_tpr.std(axis=0)\n",
    "    whole_D_tprs_upper = np.minimum(mean_whole_D_tpr + std_whole_D_tpr, 1)\n",
    "    whole_D_tprs_lower = mean_whole_D_tpr - std_whole_D_tpr\n",
    "\n",
    "\n",
    "    plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    A_plt, = plt.plot(base_fpr, mean_whole_A_tpr, 'orangered', linewidth=2.0)\n",
    "    plt.fill_between(base_fpr, whole_A_tprs_lower, whole_A_tprs_upper,\n",
    "                     color='darkorange',\n",
    "                     alpha=0.3)\n",
    "\n",
    "    B_plt, = plt.plot(base_fpr, mean_whole_B_tpr, 'royalblue', linewidth=2.0)\n",
    "    plt.fill_between(base_fpr, whole_B_tprs_lower, whole_B_tprs_upper,\n",
    "                     color='lightblue',\n",
    "                     alpha=0.5)\n",
    "\n",
    "    C_plt, = plt.plot(base_fpr, mean_whole_C_tpr, 'forestgreen', linewidth=2.0)\n",
    "    plt.fill_between(base_fpr, whole_C_tprs_lower, whole_C_tprs_upper,\n",
    "                     color='limegreen',\n",
    "                     alpha=0.2)\n",
    "\n",
    "    D_plt, = plt.plot(base_fpr, mean_whole_D_tpr, 'purple', linewidth=2.0)\n",
    "    plt.fill_between(base_fpr, whole_D_tprs_lower, whole_D_tprs_upper,\n",
    "                     color='lightpink',\n",
    "                     alpha=0.2)\n",
    "\n",
    "\n",
    "    random = plt.plot([0, 1], [0, 1],'b--')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.xlabel('1-Specificity')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.grid(visible=True, which='major', color='grey', linestyle='-.')\n",
    "    plt.legend([A_plt, B_plt, C_plt, D_plt,  AnyObject()], \n",
    "               ['  EHR Data Only',\n",
    "                '  EHR and Genetic Data',\n",
    "    #             '  CV-VA (AUC: 0.8023 Â± 0.0011)',\n",
    "                '  EHR and Survey Data',\n",
    "                '  EHR Survey and Genetic Data',\n",
    "                '    +/-STD'], \n",
    "               handler_map={AnyObject: data_handler()}, loc = 'lower right', prop={'size': 13})\n",
    "    # plt.axes().set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function that loops through the other commands to perform cross validation\n",
    "\n",
    "def CrossValidation(n, k, geneDf, surveyDf, \n",
    "                    GoldStandardCohort, \n",
    "                    genePredVars):\n",
    "    \n",
    "    surveyResultsDf = pd.DataFrame({'Sensitivity': pd.Series(dtype='float'),\n",
    "                   'Specificity': pd.Series(dtype='float'),\n",
    "                   'AUC ROC Value': pd.Series(dtype='float'),\n",
    "                    'PPV': pd.Series(dtype='float')})\n",
    "    geneResultsDf = pd.DataFrame({'Sensitivity': pd.Series(dtype='float'),\n",
    "                   'Specificity': pd.Series(dtype='float'),\n",
    "                   'AUC ROC Value': pd.Series(dtype='float'),\n",
    "                    'PPV': pd.Series(dtype='float')})\n",
    "    goldStandardResultsDf = pd.DataFrame({'Sensitivity': pd.Series(dtype='float'),\n",
    "                   'Specificity': pd.Series(dtype='float'),\n",
    "                   'AUC ROC Value': pd.Series(dtype='float'),\n",
    "                    'PPV': pd.Series(dtype='float'),\n",
    "                                         'EHR AUC ROC Value': pd.Series(dtype='float')})\n",
    "    \n",
    "    surveyModelList = []\n",
    "    geneModelList = []\n",
    "    \n",
    "    shapDf = pd.DataFrame({'Survey_Shap_Values': pd.Series(dtype='float')\n",
    "                           #,'Genetic_Shap_Values': pd.Series(dtype='float')\n",
    "                          })\n",
    "                          \n",
    "    # Take all of the data sets, and break up for test and train\n",
    "    geneTestList = splitDataFrame(geneDf, n)\n",
    "    surveyTestList = splitDataFrame(surveyDf, n)\n",
    "    GoldStandardList = splitDataFrame(GoldStandardCohort, k)\n",
    "    \n",
    "    #Run an iterative Loop to train all of the models based on different sets of training and test data\n",
    "    \n",
    "    #Make a duplicate variable so that you are iterating through both genetic and survey differently\n",
    "    m = n\n",
    "    \n",
    "    for a in range(0,n):\n",
    "        for b in range(0,m):\n",
    "            \n",
    "        \n",
    "            #train every model based on the chosen set of training and testing data\n",
    "            print(a)\n",
    "            \n",
    "            geneTest, geneTrain = selectTestAndTrain(geneTestList,a,n)\n",
    "            surveyTest, surveyTrain = selectTestAndTrain(surveyTestList, b, n)\n",
    "        \n",
    "            #Train the genetic Data\n",
    "            geneResults, geneModel = TrainGeneticData(geneTest, geneTrain, genePredVars)\n",
    "            sens,spec,roc, ppv = geneResults\n",
    "            geneResultsDf = pd.concat([geneResultsDf, \n",
    "                                       pd.DataFrame.from_dict({'Sensitivity': [sens], 'Specificity':[spec], 'AUC ROC Value': [roc], 'PPV': [ppv] })], ignore_index = True)\n",
    "            \n",
    "            #Train SurveyData\n",
    "            surveyResults, surveyModel = TrainSurveyData(surveyTest, surveyTrain)\n",
    "            sens,spec,roc, ppv = surveyResults\n",
    "            surveyResultsDf = pd.concat([geneResultsDf, \n",
    "                                         pd.DataFrame.from_dict({'Sensitivity': [sens], 'Specificity':[spec], 'AUC ROC Value': [roc], 'PPV': [ppv] })], ignore_index = True)\n",
    "            \n",
    "            surveyModelList = surveyModelList + [surveyModel]\n",
    "            geneModelList = geneModelList +[geneModel]\n",
    "                                           \n",
    "                                           \n",
    "            #Find combined Results using an iteration through the goldStandardCohort\n",
    "            \n",
    "            \n",
    "            for i in range (0,k):\n",
    "                notNeededDf, GoldStandardSelect = selectTestAndTrain(GoldStandardList,i,k)\n",
    "                #Don't use notNeededDf, but use the GoldStandardSelect to evaluate the models\n",
    "                surveyPredVars = list(surveyTrain.columns)\n",
    "                surveyPredVars.remove('long_covid')\n",
    "                \n",
    "                goldsStandardResult = goldStandardEvaluation(genePredVars, surveyPredVars, \n",
    "                geneModel, surveyModel, GoldStandardSelect)\n",
    "                \n",
    "                sens,spec,roc, ppv, EHRroc, shapAppend = goldsStandardResult\n",
    "                \n",
    "                goldStandardResultsDf = pd.concat([goldStandardResultsDf, \n",
    "                                                   pd.DataFrame.from_dict({'Sensitivity': [sens], 'Specificity':[spec], 'AUC ROC Value': [roc], 'PPV': [ppv],  'EHR AUC ROC Value': [EHRroc] })], ignore_index = True)\n",
    "                \n",
    "                shapDf = pd.concat([shapDf, shapAppend], ignore_index=True)\n",
    "               \n",
    "        \n",
    "        \n",
    "        \n",
    "    curveWeighted(geneModelList, surveyModelList, GoldStandardList, genePredVars, surveyPredVars,k)\n",
    "    \n",
    "    return surveyResultsDf, geneResultsDf, goldStandardResultsDf, surveyModelList, geneModelList, shapDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function that loops through the other commands to perform cross validation\n",
    "\n",
    "def CrossValidationShort(n, k, geneDf, surveyDf, \n",
    "                    surveryModelList, geneModelList, GoldStandardCohort,\n",
    "                    genePredVars):\n",
    "    \n",
    "    goldStandardResultsDf = pd.DataFrame({'Sensitivity': pd.Series(dtype='float'),\n",
    "                   'Specificity': pd.Series(dtype='float'),\n",
    "                   'AUC ROC Value': pd.Series(dtype='float'),\n",
    "                    'PPV': pd.Series(dtype='float'),\n",
    "                                         'EHR AUC ROC Value': pd.Series(dtype='float')})\n",
    "   \n",
    "    shapDf = pd.DataFrame({'Survey_Shap_Values': pd.Series(dtype='float')\n",
    "                           #,'Genetic_Shap_Values': pd.Series(dtype='float')\n",
    "                          })\n",
    "                          \n",
    "    # Take all of the data sets, and break up for test and train\n",
    "    geneTestList = splitDataFrame(geneDf, n)\n",
    "    surveyTestList = splitDataFrame(surveyDf, n)\n",
    "    GoldStandardList = splitDataFrame(GoldStandardCohort, k)\n",
    "    \n",
    "    #Run an iterative Loop to train all of the models based on different sets of training and test data\n",
    "    \n",
    "    #Make a duplicate variable so that you are iterating through both genetic and survey differently\n",
    "    m = n\n",
    "    \n",
    "    for a in range(0,n):\n",
    "        for b in range(0,m):\n",
    "            \n",
    "        \n",
    "            #train every model based on the chosen set of training and testing data\n",
    "            print(a)\n",
    "            \n",
    "            geneModel = geneModelList[a]\n",
    "            surveyModel = surveyModelList[b]\n",
    "            \n",
    "            \n",
    "            for i in range (0,k):\n",
    "                notNeededDf, GoldStandardSelect = selectTestAndTrain(GoldStandardList,i,k)\n",
    "                #Don't use notNeededDf, but use the GoldStandardSelect to evaluate the models\n",
    "                surveyPredVars = list(surveyDf.columns)\n",
    "                surveyPredVars.remove('long_covid')\n",
    "                #surveyPredVars.remove('person_id')\n",
    "                \n",
    "                goldsStandardResult = goldStandardEvaluation(genePredVars, surveyPredVars, \n",
    "                geneModel, surveyModel, GoldStandardSelect)\n",
    "                \n",
    "                sens,spec,roc, ppv, EHRroc, shapAppend = goldsStandardResult\n",
    "                \n",
    "                goldStandardResultsDf = pd.concat([goldStandardResultsDf, \n",
    "                                                   pd.DataFrame.from_dict({'Sensitivity': [sens], 'Specificity':[spec], 'AUC ROC Value': [roc], 'PPV': [ppv],  'EHR AUC ROC Value': [EHRroc] })], ignore_index = True)\n",
    "                \n",
    "                shapDf = pd.concat([shapDf, shapAppend], ignore_index=True)\n",
    "\n",
    "    \n",
    "    return goldStandardResultsDf, shapDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import csv files of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n3cCohort = getFromBucket('n3c_aou_cohort_ft_May_23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "surveyData = getFromBucket('SurveyDataDfMay17.csv')\n",
    "\n",
    "drop_list = ['Race: What Race Ethnicity', 'The Basics: Sexual Orientation',\n",
    "             'Home Own: Current Home Own','Income: Annual Income','Education Level: Highest Grade','Living Situation: Stable House Concern']\n",
    "surveyDataDf = surveyData.drop(drop_list, axis=1)\n",
    "surveyDataDf = surveyDataDf[surveyDataDf['person_id'].isin(n3cCohort['person_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GeneticDataDf= getFromBucket('GeneticDataDfAugust15.csv')\n",
    "GeneticDataDf= GeneticDataDf[GeneticDataDf['person_id'].isin(n3cCohort['person_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GoldStandardCohort = getFromBucket('GoldStandardAllDataAugust15.csv')\n",
    "GoldStandardCohort =  GoldStandardCohort.drop('min_covid_dt', axis = 1)\n",
    "GoldStandardCohort= GoldStandardCohort[GoldStandardCohort['person_id'].isin(n3cCohort['person_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "genePredVars = ['chr1:155127096:C:T_T',\n",
    " 'chr1:155203736:G:A_A', 'chr3:45793925:G:A_A', 'chr3:45838989:T:C_C', 'chr3:101780431:T:C_C',\n",
    " 'chr6:31153455:T:C_C', 'chr6:33076153:A:G_G', 'chr6:41522644:G:C_C', 'chr6:41515652:G:C_C',\n",
    "                'chr9:133273813:C:T_C',\n",
    " 'chr10:79946568:A:G_G', 'chr11:1219991:G:T_T', 'chr11:34507219:C:A_A', 'chr11:34507219:C:T_T',\n",
    " 'chr12:112914354:T:C_T', 'chr12:112936943:C:T_C', 'chr12:132564254:T:C_T', 'chr16:89196249:G:A_A',\n",
    " 'chr17:45707983:T:C_C', 'chr17:49863303:C:T_T', 'chr19:4719431:G:A_A', 'chr19:10355447:C:T_T',\n",
    " 'chr19:48867352:G:T_T', 'chr19:50379362:T:C_C','chr21:33242905:T:C_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "features = genePredVars + ['long_covid', 'person_id', 'y_pred']\n",
    "GeneticDataDf = GeneticDataDf[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = GeneticDataDf.loc[GeneticDataDf['long_covid']==1, 'person_id']\n",
    "b = surveyDataDf.loc[surveyDataDf['long_covid']==1, 'person_id']\n",
    "c = GoldStandardCohort.loc[GoldStandardCohort['long_covid']==1, 'person_id']\n",
    "\n",
    "\n",
    "df_list = [a,b,c]\n",
    "AllDfs=pd.concat(df_list,axis=1)\n",
    "AllDfs = AllDfs.T.drop_duplicates().T\n",
    "print(len(AllDfs))\n",
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GoldStandardCohort[GoldStandardCohort['person_id'].isin(GeneticDataDf['person_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the functions to run the results above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = CrossValidationSimple(3, 5, GeneticDataDf, surveyDataDf, genePredVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullResults  = CrossValidation(5, 4, GeneticDataDf, surveyDataDf, GoldStandardCohort, genePredVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveyResults, geneResults,  gsd, surveyModelList, geneModelList, shapDf = fullResults  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gsd['PPV']\n",
    "\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gsd['Sensitivity']\n",
    "\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gsd['AUC ROC Value']\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add standard deviation for average metrics\n",
    "a = gsd['Specificity']\n",
    "\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsd.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveyPredVars = list(surveyDataDf.columns)\n",
    "surveyPredVars.remove('long_covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withNaTrain = GoldStandardCohort[surveyPredVars+['long_covid']]\n",
    "withNaTrain = withNaTrain.drop(['person_id', 'y_pred'], axis=1)\n",
    "    \n",
    "yTrain = withNaTrain['long_covid']\n",
    "xTrain = withNaTrain.loc[:, withNaTrain.columns !='long_covid']\n",
    "xTrain = xTrain.loc[:, xTrain.columns !='y_pred']\n",
    "\n",
    "    \n",
    "train = xgb.DMatrix(xTrain, label = yTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=surveyModelList[0]\n",
    "\n",
    "#train = GoldStandardCohort\n",
    "\n",
    "pred = model.predict(train, output_margin=True)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(train)\n",
    "\n",
    "\n",
    "#shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgboost.plot_importance(model, height = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(xTrain)\n",
    "#shap.summary_plot(shap_values, xTrain, plot_type = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(surveyModelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveyModelList = surveyModelList[0:5]\n",
    "geneModelList = [geneModelList[0]]+[geneModelList[5]]+[geneModelList[10]]+[geneModelList[15]]+[geneModelList[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n3c_model = xgboost.XGBClassifier()\n",
    "#n3c_model.load_model('long_covid_model.ubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Genetic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Importance Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create n3c_importances from their manuscript\n",
    "\n",
    "n3c_importances = [1250.9,113.86, 353.85, 139.28, 115.59, 144.05, 254.54, 42.22, 79.19, 39.64, 75.89, 49.16, 89.02, 66.24, 62.61, 43.03, 42.01, 53.18, 50.27, 38.14]\n",
    "n3c_importances_prop = []\n",
    "for a in n3c_importances:\n",
    "    prop = a / sum(n3c_importances)\n",
    "    prop = prop * (0.5 / 0.9)\n",
    "    n3c_importances_prop = n3c_importances_prop + [prop]\n",
    "\n",
    "n3c_features = ['post_covid_outpatient_utilisation','difficulty_breathing','age','dysponoea','male_sex','covid_vaccine_med','post_covid_inpatient_utilisation','polyethylene_gycol_350_med', 'albuterol_med','chronic_pulmonary_disease','dexamethasone_med','hyperlipidaemia','pre_existing_diabetes','metoprolol_med','chronic_kidney_disease','naloxone_med','backache','melatonin_med','hospitalized_for_covid','propofol_med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n3c_features[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shapDf.iloc[0,0][0]\n",
    "shap_values = np.concatenate((shapDf.iloc[0,0][1],shapDf.iloc[0,0][0]))\n",
    "#shap.summary_plot(shap_values, xTrain, plot_type = \"bar\", feature_names = feature_names)\n",
    "#shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = xTrain.columns\n",
    "\n",
    "\n",
    "rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "vec_vals = rf_resultX.values.mean(0)\n",
    "\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "\n",
    "shap_direction = pd.DataFrame(list(zip(feature_names, vec_vals)),\n",
    "                                  columns=['col_name','vector_value'])\n",
    "\n",
    "survey_sum  = sum(shap_importance['feature_importance_vals'])\n",
    "shap_importance['feature_importance_vals'] = shap_importance['feature_importance_vals'] * (0.4 /survey_sum )\n",
    "shap_direction['vector_value'] = shap_direction['vector_value'] * (0.4 /survey_sum )\n",
    "\n",
    "for a in range(0,20):\n",
    "    x=1\n",
    "    shap_importance = pd.concat([shap_importance, pd.DataFrame.from_dict({'col_name': [n3c_features[a]], 'feature_importance_vals': [n3c_importances_prop[a]] })],ignore_index=True)\n",
    "    \n",
    "    shap_direction = pd.concat([shap_direction, pd.DataFrame.from_dict({'col_name': [n3c_features[a]], 'vector_value': [0.0] })],ignore_index=True)\n",
    "shap_importance.sort_values(by=['feature_importance_vals'],\n",
    "                               ascending=False, inplace=True)\n",
    "\n",
    "shap_direction['vector_value'] = shap_direction['vector_value'] * 100.0\n",
    "\n",
    "shap_importance\n",
    "\n",
    "#pd.concat([geneResultsDf, pd.DataFrame.from_dict({'Sensitivity': [sens], 'Specificity':[spec], 'AUC ROC Value': [roc], 'PPV': [ppv] })], ignore_index = True)\n",
    "            \n",
    "\n",
    "#sum(shap_importance['feature_importance_vals'])\n",
    "#1.4153312656999333 is the total of the survey shap values, so we devide to get a proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_direction.sort_values(by=['vector_value'],\n",
    "                               ascending=False, inplace=True)\n",
    "shap_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_importance.reset_index(drop=True).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert the dataframe so that biggest figures are on the top\n",
    "shap_importance = shap_importance.head(20).sort_values('feature_importance_vals', ascending = True)\n",
    "feature_importance_df = shap_importance\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_direction_20 = shap_direction[shap_direction['col_name'].isin(feature_importance_df['col_name'])].reset_index(drop=True)\n",
    "shap_direction_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this function instead for different colors\n",
    "#Need to update the colors\n",
    "my_colors = ['r','r','b','b','b','purple', 'b','r','b','b','purple','r','r','b','r','r','purple','r','r','r']\n",
    "sgigray32, sgigray52, sgigray72, sgigray92 = ['#515151', '#848484', '#B7B7B7', '#EAEAEA']\n",
    "my_grey_scale = [sgigray32,sgigray32,sgigray52,sgigray52,sgigray52,sgigray72, sgigray52,sgigray32,sgigray52,sgigray52,sgigray72,sgigray32,sgigray32,sgigray52,sgigray32,sgigray32,sgigray72,sgigray32,sgigray32,sgigray32]\n",
    "graph_df = shap_importance.head(20)\n",
    "\n",
    "features = graph_df['col_name']\n",
    "importances = graph_df['feature_importance_vals']\n",
    "graph_df.plot.barh(x = 'col_name', y = 'feature_importance_vals', color = my_grey_scale,\n",
    "                   width = 0.9, figsize = (10,10),\n",
    "                   title='Feature Importance in Overall Predictive Method',  #hatch='//'\n",
    "                  )\n",
    "plt.xlabel('Relative Proportionate Contribution')\n",
    "plt.ylabel('Feature Names')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = pd.DataFrame({'EHR Variable': 1,'Survey Variable':2,'Mobile Device Variable': 4, 'Genetic Variable': 10}, index = [\"n\"])\n",
    "example_df.plot( kind='bar', color = [sgigray32,sgigray52,sgigray72,sgigray92], #hatch='//'\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature_importance_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viridis = cm.get_cmap('viridis', 256)\n",
    "#top = cm.get_cmap('Oranges_r', 128)\n",
    "#bottom = cm.get_cmap('Blues', 128)\n",
    "#newcolors = np.vstack((top(np.linspace(0, 1, 128)), bottom(np.linspace(0, 1, 128))))\n",
    "#newcmp = ListedColormap(newcolors, name='OrangeBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coeff_value_ = {}\n",
    "    for key, value in correlation_coeff_value.items():\n",
    "        correlation_coeff_value_[key] = np.mean(value)\n",
    "    corr_df = pd.DataFrame.from_dict(correlation_coeff_value_, orient='index').fillna(0)\n",
    "    corr_df.reset_index(inplace=True)\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    color_assigned = []\n",
    "    for corr in list(corr_df['Corr']):\n",
    "        if not np.isnan(corr):\n",
    "            color_assigned.append(colors[int((corr+1)/2 * len(colors))])\n",
    "        else:\n",
    "            color_assigned.append([1,1,1,1])\n",
    "    corr_df['Sign'] = color_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeShapValues(modelList,n, modelType):\n",
    "    returnList = []\n",
    "    if modelType == 'gene':\n",
    "        test =  removeNonNumeric(GoldStandardCohort[genePredVars+['long_covid']])\n",
    "        test = test.drop('long_covid', axis = 1)\n",
    "        \n",
    "        for a in range(0,n):\n",
    "            model = modelList[a]\n",
    "            explainer = shap.KernelExplainer(model.predict, test)\n",
    "            shap_values = explainer.shap_values(test, nsamples=100)\n",
    "            #shap.summary_plot(shap_values, test, plot_type = \"bar\")\n",
    "            \n",
    "            returnList=  returnList + [shap_values]\n",
    "        \n",
    "    if modelType == 'survey':\n",
    "        withNaTest = GoldStandardCohort[surveyPredVars+['long_covid']]\n",
    "        withNaTest = withNaTrain.drop('y_pred', axis=1)\n",
    "    \n",
    "        yTest = withNaTrain['long_covid']\n",
    "        xTest = withNaTrain.loc[:, withNaTrain.columns !='long_covid']\n",
    "        xTest = xTrain.loc[:, xTrain.columns !='y_pred']\n",
    "        test = xgb.DMatrix(xTrain, label = yTrain)\n",
    "\n",
    "    return returnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneShapValues = makeShapValues([geneModelList[0]],1, 'gene')\n",
    "geneShapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to interpret these values\n",
    "feature_names = genePredVars\n",
    "\n",
    "\n",
    "rf_resultX = pd.DataFrame(geneShapValues[0], columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "#vals = rf_resultX.values.mean(0)\n",
    "\n",
    "gene_shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "gene_sum = sum(gene_shap_importance['feature_importance_vals'])\n",
    "gene_shap_importance['feature_importance_vals'] = gene_shap_importance['feature_importance_vals'] * (0.1 / gene_sum)\n",
    "gene_shap_importance = gene_shap_importance.sort_values('feature_importance_vals', ascending = False)\n",
    "gene_shap_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surveyPredVars = list(surveyDataDf.columns)\n",
    "#surveyPredVars = surveyPredVars.drop(['person_id','long_covid'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "test_pred_vals = getWeightedPredValues(0.07, 0.23, genePredVars, surveyPredVars, geneModelList[a], surveyModelList[a], removeNonNumeric(GoldStandardCohort))\n",
    "createAUROC(test_pred_vals, removeNonNumeric(GoldStandardCohort)['long_covid'], \"modelLabel\")\n",
    "#GeneticDataDf, surveyDataDf, genePredVars, surveyModelList, geneModelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GoldStandardCohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a set of functions that will run a fairness Analysis on a given variable\n",
    "\n",
    "#Parameters:\n",
    "##test_df: data frame with all survey data, including the fairness variable in question\n",
    "##fair_var: The variable we want to assess fairness over\n",
    "##fair_var_values = [1] : these are the values we want to consider as part of a group to determine fairness from.\n",
    "### for instance, if we are evaluating fairness on education fair_var_values = [college education, doctorate or professional degree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairnessAnalysis(test_df, fair_var, fair_var_values = [1], column_drop_list = [], skip_list = [0]):\n",
    "    \n",
    "    #First, we need to seperate the test df based on fairness variables\n",
    "    fairness_categories = [1,2]\n",
    "    fairness_categories[0] = test_df.loc[test_df[fair_var].isin(fair_var_values), :]\n",
    "    other_values = test_df[fair_var].unique().tolist()\n",
    "    for a in fair_var_values:\n",
    "        other_values.remove(a)\n",
    "    fairness_categories[1] = test_df.loc[test_df[fair_var].isin(other_values), :]\n",
    "    \n",
    "    #Now that we have split the dataframe based on these variables, we need to remove them before we use our ML algorithm\n",
    "    \n",
    "    for i in [0,1]:\n",
    "        fairness_categories[i] = fairness_categories[i].drop(column_drop_list, axis = 1)\n",
    "        print(len(fairness_categories[i]))\n",
    "        \n",
    "    #Now we need to use ML on each group and see if we get different results\n",
    "    #Let's use our function to get Sensitivity, Specificity, AUROC, and shap values for each set of cases\n",
    "    fairness_results = [1,2]\n",
    "    \n",
    "    #Adapt this code to compare using the gold standard results, for more accurate and easier runtime\n",
    "    goldStandard1 = GoldStandardCohort.loc[ GoldStandardCohort['person_id'].isin(fairness_categories[0]['person_id']), :]\n",
    "    goldStandard2 = GoldStandardCohort.loc[ GoldStandardCohort['person_id'].isin(fairness_categories[1]['person_id']), :]\n",
    "    fairness_results[0] = CrossValidationShort(5, 4, GeneticDataDf, fairness_categories[0], surveyModelList, geneModelList, goldStandard1, genePredVars)\n",
    "    fairness_results[1] = CrossValidationShort(5, 4, GeneticDataDf, fairness_categories[1], surveyModelList, geneModelList, goldStandard2, genePredVars)\n",
    "    \n",
    "    return fairness_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(surveyModelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drop_list = ['Race: What Race Ethnicity', 'The Basics: Sexual Orientation',\n",
    "             'Home Own: Current Home Own','Income: Annual Income','Education Level: Highest Grade','Living Situation: Stable House Concern']\n",
    "\n",
    "sexual_fairness = fairnessAnalysis(surveyData, 'The Basics: Sexual Orientation', fair_var_values = [1], column_drop_list = drop_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " gsd, shapDf = sexual_fairness[0]\n",
    "\n",
    "a = gsd['AUC ROC Value']\n",
    "a = gsd['Specificity']\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsd,shapDf = sexual_fairness[1]\n",
    "\n",
    "a = gsd['AUC ROC Value']\n",
    "a = gsd['Specificity']\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economic_fairness = fairnessAnalysis(surveyData, 'Income: Annual Income', fair_var_values = [0,1,2,3], column_drop_list = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsd, shapDf = economic_fairness[0]\n",
    "\n",
    "a = gsd['AUC ROC Value']\n",
    "a = gsd['Specificity']\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " gsd, shapDf = economic_fairness[1]\n",
    "\n",
    "a = gsd['AUC ROC Value']\n",
    "a = gsd['Specificity']\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_fairness = fairnessAnalysis(surveyData, 'Education Level: Highest Grade', fair_var_values = [1,2,3,4,5], column_drop_list = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsd, shapDf = education_fairness[0]\n",
    "\n",
    "a = gsd['AUC ROC Value']\n",
    "a = gsd['Specificity']\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsd, shapDf = education_fairness[1]\n",
    "\n",
    "a = gsd['AUC ROC Value']\n",
    "a = gsd['Specificity']\n",
    "[a.mean() , a.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "146.992px",
    "width": "358.997px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
