{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements Here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "rng = np.random.default_rng()\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "#from venn import venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This snippet assumes that you run setup first\n",
    "\n",
    "## This code lists objects in your Google Bucket\n",
    "\n",
    "## Get the bucket name\n",
    "#my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "## List objects in the bucket\n",
    "#print(subprocess.check_output(f\"gsutil ls -r {my_bucket}\", shell=True).decode('utf-8'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFromBucket(name_of_file_in_bucket):\n",
    "    # get the bucket name\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file from the bucket to the current working space\n",
    "    os.system(f\"gsutil cp '{my_bucket}/data/{name_of_file_in_bucket}' .\")\n",
    "\n",
    "    print(f'[INFO] {name_of_file_in_bucket} is successfully downloaded into your working space')\n",
    "    # save dataframe in a csv file in the same workspace as the notebook\n",
    "    my_dataframe = pd.read_csv(name_of_file_in_bucket)\n",
    "    return my_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeInBucket(name_of_file_loc, a = 'dataFrame'):\n",
    "    # get the bucket name\n",
    "    \n",
    "    # This snippet assumes you run setup first\n",
    "\n",
    "    # This code saves your dataframe into a csv file in a \"data\" folder in Google Bucket\n",
    "\n",
    "    # Replace df with THE NAME OF YOUR DATAFRAME\n",
    "    if (type(name_of_file_loc)== type('string')):\n",
    "        my_dataframe = pd.read_csv(name_of_file_loc)\n",
    "        destination_filename = name_of_file_loc\n",
    "    else:\n",
    "        my_dataframe = name_of_file_loc\n",
    "        destination_filename = a\n",
    "\n",
    "    # Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
    "    \n",
    "\n",
    "    ########################################################################\n",
    "    ##\n",
    "    ################# DON'T CHANGE FROM HERE ###############################\n",
    "    ##\n",
    "    ########################################################################\n",
    "\n",
    "    # save dataframe in a csv file in the same workspace as the notebook\n",
    "    my_dataframe.to_csv(destination_filename, index=False)\n",
    "\n",
    "    # get the bucket name\n",
    "    my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "    # copy csv file to the bucket\n",
    "    args = [\"gsutil\", \"cp\", f\"./{destination_filename}\", f\"{my_bucket}/data/\"]\n",
    "    output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "    # print output from gsutil\n",
    "    output.stderr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Mobile Device Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InnerMobileData = getFromBucket('InnerJoinMarch16MobileDevice.csv')\n",
    "OuterMobileData = getFromBucket('InnerJoinMarch16MobileDevice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Data Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storeInBucket(\"Jan2023GeneticData.csv\")\n",
    "\n",
    "GeneticDataDf = getFromBucket(\"GeneticDataDfAugust14.csv\")\n",
    "GeneticDataDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DistributionDf = pd.DataFrame(columns = ['Genetic Variant','Percent Expression'])\n",
    "for a in range(1,37):\n",
    "    Gene = GeneticDataDf.columns[a]\n",
    "    Proportion = len(GeneticDataDf[GeneticDataDf.iloc[:,a]>0])/4803\n",
    "    df2 = {'Genetic Variant':Gene,'Percent Expression':Proportion}\n",
    "    #DistributionDf = DistributionDf.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DistributionDf = DistributionDf.drop(25)\n",
    "#DistributionDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorrDf=GeneticDataDf.corr().drop(['chr16:89196249:G:GGCCGGCTGCCTGGCCGCCCTGCCGGCCGTGCAGGGTGTCCAGGA_GGCCGGCTGCCTGGCCGCCCTGCCGGCCGTGCAGGGTGTCCAGGA'])\n",
    "CorrDf  = CorrDf.drop(['chr16:89196249:G:GGCCGGCTGCCTGGCCGCCCTGCCGGCCGTGCAGGGTGTCCAGGA_GGCCGGCTGCCTGGCCGCCCTGCCGGCCGTGCAGGGTGTCCAGGA'], axis =1)\n",
    "CorrDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in CorrDf.index:\n",
    "    for b in CorrDf.columns:\n",
    "        if (a != b):\n",
    "            if (abs(CorrDf.loc[a,b]) > 0.75):\n",
    "                print(a + \" vs \" + b + \" = \" + str(CorrDf.loc[a,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(GeneticDataDf.columns)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the extra genetic Variables, and any variables that don't show any representation in the population\n",
    "ToDropList = ['chr16:89196249:G:GGCCGGCTGCCTGGCCGCCCTGCCGGCCGTGCAGGGTGTCCAGGA_GGCCGGCTGCCTGGCCGCCCTGCCGGCCGTGCAGGGTGTCCAGGA',\n",
    "             'chr3:45859597:C:T_T','chr3:101705614:T:C_C', 'chr3:101705614:TG:T_T' , 'chr6:31153649:G:A_A','chr6:41534945:A:C_C',\n",
    "              'chr11:1219991:G:A_A', 'chr11:1219991:G:C_C', 'chr12:132564254:T:G_G','chr16:89196249:G:T_T', 'chr17:45707983:T:G_G',\n",
    "              'chr21:33252612:A:G_A']\n",
    "\n",
    "GeneticDataDf = GeneticDataDf.drop(ToDropList , axis = 1)\n",
    "#GeneticDataDf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorrDf=GeneticDataDf.corr()\n",
    "CorrDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in CorrDf.index:\n",
    "    for b in CorrDf.columns:\n",
    "        if (a != b):\n",
    "            if (abs(CorrDf.loc[a,b]) > 0.75):\n",
    "                print(a + \" vs \" + b + \" = \" + str(CorrDf.loc[a,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in n3c cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n3cCohort = getFromBucket('n3c_aou_cohort_ft_May_23.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in EHR, survey data and Long Covid Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Long Covid EHR data from google bucket\n",
    "LongCovidEHRDf = getFromBucket('n3c_aou_cohort.csv')\n",
    "#for a in LongCovidEHRDf.columns:\n",
    "    #print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Survey Data from google bucket\n",
    "SurveyDataDF = getFromBucket('SurveyDataMarch2023.csv')\n",
    "SurveyDataDF= SurveyDataDF.sort_values('person_id')\n",
    "drop_list = ['Race: What Race Ethnicity', 'The Basics: Sexual Orientation',\n",
    "             'Home Own: Current Home Own','Income: Annual Income','Education Level: Highest Grade','Living Situation: Stable House Concern']\n",
    "SurveyDataDF = SurveyDataDF.drop(drop_list, axis=1)\n",
    "SurveyDataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in Genetic Data from google bucket\n",
    "#currently this data is loaded in from the same notebook, so this cell is irrelevant until we do our sorting later on\n",
    "\n",
    "#GeneticDataDf = getFromBucket('GeneticDataFeb2023.csv')\n",
    "#GeneticDataDf= GeneticDataDf.sort_values('person_id')\n",
    "#GeneticDataDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Variables from Genetic data, Survey data, and EHR data to include in the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectVariables(df, variables):\n",
    "    returnDf = df.loc[:, variables]\n",
    "    returnDf=returnDf.sort_values('person_id', ignore_index=True)\n",
    "    return returnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We select the data variables that we want from our EHR data added to our dataframe here\n",
    "\n",
    "EHRAppendDf = selectVariables( LongCovidEHRDf, ['y_pred','person_id','long_covid'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We select the data variables that we want from our survey data added to our dataframe here\n",
    "\n",
    "nonDuplicatesVariables = []\n",
    "\n",
    "for a in SurveyDataDF.columns:\n",
    "    if a in GeneticDataDf.columns:\n",
    "        print(a)\n",
    "    else: \n",
    "        nonDuplicatesVariables = nonDuplicatesVariables+ [a]\n",
    "        \n",
    "nonDuplicatesVariables = nonDuplicatesVariables+ ['person_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveyAppendDf = selectVariables(SurveyDataDF, nonDuplicatesVariables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonNumeric(df):\n",
    "    #df = df[df['long_covid'] > - 1]\n",
    "    df = df.replace('NaN', np.NaN)\n",
    "    df = df.replace('nan', np.NaN)\n",
    "    for a in df.columns:\n",
    "    #columnDf = trialDf[a]\n",
    "        columnMean = df.loc[df[a].notna() ,a].mean()\n",
    "    #print(a)\n",
    "    #print(columnMean)\n",
    "        df[a] = df[a].fillna(value = columnMean)\n",
    "    returnDf = df\n",
    "    return returnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We select the data variables that we want from our genetic data added to our dataframe here\n",
    "GeneticDataDf = removeNonNumeric(GeneticDataDf)\n",
    "geneAppendDf = selectVariables(GeneticDataDf, GeneticDataDf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GeneticDataDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a gold standard cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are going to join just survey data and mobile device data\n",
    "\n",
    "def joinMobileDeviceWithSurveyData(mobileDf, surveyDf, mergeType = 'inner'):\n",
    "    \n",
    "    returnDf = pd.merge(left = mobileDf, right = surveyDf, how = mergeType, left_on = 'person_id', right_on='person_id')\n",
    "    \n",
    "    return returnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinMobileDeviceWithGeneticData(mobileDf, geneAppendDf, mergeType = 'inner'):\n",
    "    \n",
    "    returnDf = pd.merge(left = mobileDf, right = geneAppendDf, how = mergeType, left_on = 'person_id', right_on='person_id')\n",
    "    \n",
    "    return returnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoinSurveyMobileDevice = joinMobileDeviceWithSurveyData(InnerMobileData, surveyAppendDf, mergeType = 'outer')\n",
    "innerJoinAllData = joinMobileDeviceWithGeneticData(innerJoinSurveyMobileDevice, geneAppendDf, mergeType = 'inner')\n",
    "innerJoinAllData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldStandardPersons = innerJoinAllData['person_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine data from all data sources by person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Start here and go down######\n",
    "#We want to include the alterations to the survey data df\n",
    "#Make a new machineLearningDf based on survey, genetic, and mobile device data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a bunch of dataframes, and joins them by the joining variable\n",
    "    #duplicate columns will be dropped at the end, \n",
    "    #so if there are still duplicate column names, that means some of the columns don't match\n",
    "def combineData(dataFrameList, joinVariable):\n",
    "    joiningSet = dataFrameList[0].loc[:,joinVariable]\n",
    "    \n",
    "    \n",
    "    #Now redefine joiningSet by going through each dataframe in the list \n",
    "    #and redefine as only the joiningVariables that are both in that dataframe, and the previous joiningSet\n",
    "    for df in dataFrameList:\n",
    "        joiningSet = df.loc[df[joinVariable].isin(joiningSet),joinVariable]\n",
    "    \n",
    "    \n",
    "    #now that we have a joiningSet that includes only the common part of every data set, \n",
    "    #we need to restrict each data frame according to that set\n",
    "    \n",
    "    cleanedDfList = []\n",
    "    for df in dataFrameList:\n",
    "        cleanedDf = df.loc[df[joinVariable].isin(joiningSet),:].sort_values(joinVariable, ignore_index=True)\n",
    "        cleanedDfList = cleanedDfList + [cleanedDf]\n",
    "        \n",
    "    #now join them all together with a concat statement and remove duplicate columns\n",
    "    returnDf=pd.concat(cleanedDfList,axis=1)\n",
    "    returnDf = returnDf.T.drop_duplicates().T\n",
    "    return returnDf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machineLearningDf = combineData([EHRAppendDf, surveyAppendDf, geneAppendDf], 'person_id')\n",
    "machineLearningDf= removeNonNumeric(machineLearningDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the simplifying assumption that takes the data and changes the long_covid valuies of high y_pred patients\n",
    "\n",
    "#machineLearningDf\n",
    "def simplifyingAssumption(df, p=0.9):\n",
    "    for i in range(0,len(df)):\n",
    "        if (df.loc[i,'y_pred'] >= p):\n",
    "            df.loc[i,'long_covid']=1\n",
    "        \n",
    "    return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machineLearningDf = simplifyingAssumption(machineLearningDf)\n",
    "#We want to apply the simplifying assumption to only test and train, not gold standard set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonNumeric(df):\n",
    "    df = df[df['long_covid'] > - 1]\n",
    "    df = df.replace('NaN', np.NaN)\n",
    "    df = df.replace('nan', np.NaN)\n",
    "    for a in df.columns:\n",
    "    #columnDf = trialDf[a]\n",
    "        columnMean = df.loc[df[a].notna() ,a].mean()\n",
    "    #print(a)\n",
    "    #print(columnMean)\n",
    "        df[a] = df[a].fillna(value = columnMean)\n",
    "    returnDf = df\n",
    "    return returnDf    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machineLearningDf = removeNonNumeric(machineLearningDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacent: Combine data from Survey data and Mobile device data in a seperate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we make a dataframe of just survey data and mobile device data with TONS of empty values\n",
    "#We are going to apply this to a different algorithm that can handle these empty values\n",
    "outerJoinSurveyMobileDevice = joinMobileDeviceWithSurveyData(OuterMobileData, surveyAppendDf, mergeType = 'outer')\n",
    "#outerJoinSurveyMobileDevice= simplifyingAssumption(outerJoinSurveyMobileDevice)\n",
    "outerJoinSurveyMobileDevice = outerJoinSurveyMobileDevice.drop('min_covid_dt', axis=1)\n",
    "outerJoinSurveyMobileDevice = removeNonNumeric(outerJoinSurveyMobileDevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outerJoinSurveyMobileDevice = outerJoinSurveyMobileDevice.drop(['Race: What Race Ethnicity', 'The Basics: Sexual Orientation'], axis=1)\n",
    "#outerJoinSurveyMobileDevice.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control model Data Frame with ALL data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append EHR, Genetic, Survey, and Mobile Device Data into a big dataframe with lots of wholes\n",
    "\n",
    "#EHR Dataframe\n",
    "LongCovidEHRDf = LongCovidEHRDf.sort_values('person_id')\n",
    "\n",
    "#Survey Dataframe\n",
    "SurveyDataDF = SurveyDataDF.sort_values('person_id')\n",
    "\n",
    "#Genetic Dataframe\n",
    "GeneticDataDf = GeneticDataDf.sort_values('person_id')\n",
    "\n",
    "#Mobile Device Dataframe\n",
    "OuterMobileData = OuterMobileData.sort_values('person_id')\n",
    "\n",
    "#Use Existing Join function to join all the data\n",
    "step1 = joinMobileDeviceWithSurveyData(LongCovidEHRDf, SurveyDataDF, mergeType = 'outer')\n",
    "step2 = joinMobileDeviceWithSurveyData(step1, GeneticDataDf, mergeType = 'outer')\n",
    "step3 = joinMobileDeviceWithSurveyData(step2, OuterMobileData, mergeType = 'outer')\n",
    "\n",
    "ControlModelAllDataDf = step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ControlModelAllDataDf = ControlModelAllDataDf.drop(\n",
    "    ['index', 'long_covid_y','min_long_covid_date','y_pred_y']\n",
    "    , axis = 1)\n",
    "\n",
    "\n",
    "#Rename long_covid_x as long_covid\n",
    "ControlModelAllDataDf = ControlModelAllDataDf.rename(columns={'long_covid_x':'long_covid'})\n",
    "ControlModelAllDataDf = ControlModelAllDataDf.rename(columns={'y_pred_x':'y_pred'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a in ControlModelAllDataDf.columns:\n",
    "    #print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Testing and Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goldStandardPersons))\n",
    "print(len(machineLearningDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to remove gold standard persons using the person list from the gold standard cohort\n",
    "#If there's not enough genetic data to train on, we can split up the gold standard cohort and just reserve a part of it for later\n",
    "#Alter the code below....\n",
    "\n",
    "def makeTestAndTrainSets(df, trainProp):\n",
    "    #First, control for the df being in the n3c cohort\n",
    "    \n",
    "    df = df[df[\"person_id\"].isin(n3cCohort['person_id'])]\n",
    "    \n",
    "    trainDf = df[df['long_covid']==0].sample(frac=trainProp, random_state=101)\n",
    "    trainDf =pd.concat([trainDf,df[df['long_covid']==1].sample(frac=trainProp, random_state=101)], ignore_index=True)\n",
    "    \n",
    "    NotInList=[]\n",
    "    \n",
    "    testDf =  df\n",
    "\n",
    "\n",
    "    toDropList = df.index[df['person_id'].isin(trainDf['person_id'].tolist() + NotInList)]\n",
    "\n",
    "    #print(toDropList)\n",
    "    testDf = testDf.drop(index = toDropList, axis=0)\n",
    "    testDf = testDf.reset_index(drop=True)\n",
    "    return trainDf, testDf\n",
    "    #trainDf=trainDf.drop(['min_long_covid_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function twice, once to make a gold standard cohort, and the other to make training and testing data\n",
    "machineLearningDf, goldStandardDf = makeTestAndTrainSets(machineLearningDf, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we need to remove extra pieces from the training or test sets, we can use this list\n",
    "trainDf, testDf = makeTestAndTrainSets(machineLearningDf, 0.5/0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to make a test cohort of people with both survey and mobile device data\n",
    "XGBtestSet=outerJoinSurveyMobileDevice\n",
    "XGBtestSet =  XGBtestSet[XGBtestSet['person_id'].isin(goldStandardDf['person_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outerJoinSurveyMobileDevice) - len(XGBtestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the simplifying assumption to the test and training data, but not the gold standard set\n",
    "trainDf = simplifyingAssumption(trainDf)\n",
    "testDf = simplifyingAssumption(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goldStandardDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create training and test data for the control model (all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove anywhere where longcovid is null or NA\n",
    "ControlModelAllDataDf = ControlModelAllDataDf.dropna(subset = ['person_id', 'long_covid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ControlModelAllDataDf\n",
    "#first take out the gold standard cohort, then make training and testing data\n",
    "\n",
    "GoldStandardControl = ControlModelAllDataDf[ControlModelAllDataDf['person_id'].isin(goldStandardDf['person_id'])]\n",
    "\n",
    "all_list = list(ControlModelAllDataDf['person_id'])\n",
    "remove_list = list(goldStandardDf['person_id'])\n",
    "nonGoldControls = [i for i in all_list if i not in remove_list]\n",
    "\n",
    "print(len(nonGoldControls))\n",
    "print(len(ControlModelAllDataDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonGoldControlDf = ControlModelAllDataDf[ControlModelAllDataDf['person_id'].isin(nonGoldControls)]\n",
    "trainControlDf, testControlDf = makeTestAndTrainSets(nonGoldControlDf, 0.5/0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoldStandardControl = GoldStandardControl.reset_index(drop=True)\n",
    "GoldStandardControl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Something wrong with this evaluation set, because it has 400 extra points and turns up bad results\n",
    "GoldStandardCohort = getFromBucket('GoldStandardAllData.csv')\n",
    "\n",
    "insertColumn = machineLearningDf['chr6:41515652:G:C_C']\n",
    "GoldStandardCohort = GoldStandardCohort.join(insertColumn, how ='inner')\n",
    "#GoldStandardCohort = GoldStandardCohort\n",
    "#GoldStandardCohort = removeNonNumeric(GoldStandardCohort)\n",
    "\n",
    "#We need to take out NAN values for the new gene and re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some reason, there is no overlap in person_id's from the old cohort and the new cohort\n",
    "#figure out why this is\n",
    "\n",
    "GoldStandardCohort['person_id']\n",
    "\n",
    "#GoldStandardControl['person_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storeInBucket(machineLearningDf, a = 'GeneticDataDfAugust15.csv')\n",
    "storeInBucket(GoldStandardCohort, a = 'GoldStandardAllDataAugust15.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(train, test, predictorVars,targetVar, model):\n",
    "    x=train.loc[:,predictorVars]\n",
    "    y=train[targetVar]\n",
    "    xTest=test.loc[:,predictorVars]\n",
    "    yTest=test[targetVar]\n",
    "\n",
    "    regressor = model\n",
    "    regressor.fit(x, y) \n",
    "    \n",
    "    print(\"R-Squared Train:\",metrics.r2_score(y, regressor.predict(x)))\n",
    "    print(\"R-Squared Test:\",metrics.r2_score(yTest, regressor.predict(xTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePredValues(train, test, model, predVars):\n",
    "    \n",
    "    regressor = model\n",
    "    regressor.fit(train.loc[:,predVars], train[target])\n",
    "    averageValues = []\n",
    "    nonRounded = []\n",
    "    predictions = regressor.predict(test.loc[:,predVars])\n",
    "    #print(len(testDf))\n",
    "    \n",
    "    for i in range(0,len(testDf)):\n",
    "        modelPred = predictions[i]\n",
    "        \n",
    "        if float(modelPred) <=0:\n",
    "            modelPred = 0\n",
    "        if float(modelPred) >=1:\n",
    "            modelPred = 1\n",
    "        roundNum  = round(modelPred,0)\n",
    "        averageValues = averageValues +[roundNum]\n",
    "        nonRounded = nonRounded +[modelPred]\n",
    "        \n",
    "    predValues=averageValues\n",
    "        \n",
    "    return predValues, nonRounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedAverage(trainDf, testDf, model,predictorVariables, p):\n",
    "    q = 1-p\n",
    "    regressor = model\n",
    "    regressor.fit(trainDf.loc[:,predictorVariables], trainDf[target])\n",
    "    averageValues = []\n",
    "    nonRounded = []\n",
    "    predictions = regressor.predict(testDf.loc[:,predictorVariables])\n",
    "    #print(len(testDf))\n",
    "    \n",
    "    for i in range(0,len(testDf)):\n",
    "        modelPred = predictions[i]\n",
    "        weightedAverage = (q*(modelPred)+p*(testDf.loc[i,'y_pred']))\n",
    "        \n",
    "        if float(weightedAverage) <= 0:\n",
    "            weightedAverage = 0\n",
    "        if float(weightedAverage) >=1:\n",
    "            weightedAverage = 1\n",
    "            \n",
    "        roundNum = round(weightedAverage,0)\n",
    "        \n",
    "        averageValues = averageValues +[roundNum]\n",
    "        nonRounded = nonRounded + [weightedAverage]\n",
    "        \n",
    "    predValues=averageValues\n",
    "        \n",
    "    return predValues, nonRounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triWeightedAverage(trainDf, testDf, model1, model2,genePredictorVariables, surveyPredictorVariables, p,q):\n",
    "    x = 1-(p+q)\n",
    "    regressor1 = model1\n",
    "    regressor1.fit(trainDf.loc[:,genePredictorVariables], trainDf[target])\n",
    "    \n",
    "    regressor2 = model2\n",
    "    regressor2.fit(trainDf.loc[:,surveyPredictorVariables], trainDf[target])\n",
    "    \n",
    "    averageValues = []\n",
    "    nonRounded = []\n",
    "    genePredictions = regressor1.predict(testDf.loc[:,genePredictorVariables])\n",
    "    surveyPredictions = regressor2.predict(testDf.loc[:,surveyPredictorVariables])\n",
    "    #print(len(testDf))\n",
    "    \n",
    "    for i in range(0,len(testDf)):\n",
    "        genePred = genePredictions[i]\n",
    "        surveyPred = surveyPredictions[i]\n",
    "        weightedAverage = (p*(genePred)+q*(surveyPred)+x*(testDf.loc[i,'y_pred']))\n",
    "        \n",
    "        if float(weightedAverage) <= 0:\n",
    "            roundNum = 0\n",
    "        if float(weightedAverage) >=1:\n",
    "            roundNum = 1\n",
    "        roundNum = round(weightedAverage,0)\n",
    "            \n",
    "        averageValues = averageValues +[roundNum]\n",
    "        nonRounded = nonRounded +[weightedAverage]\n",
    "    predValues=averageValues\n",
    "        \n",
    "    return predValues, nonRounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title = 'Confusion Matrix of EHR and Genetic Data Classifier'\n",
    "\n",
    "def generateConfusionMatrix(test, predValues, title):\n",
    "    confusionMatrix = metrics.confusion_matrix(test['long_covid'], predValues)\n",
    "    \n",
    "    print('Precision = ' + str(\n",
    "        (confusionMatrix[1,1])\n",
    "        /(confusionMatrix[1,1] + confusionMatrix[1,0] )))\n",
    "          \n",
    "    print('Type 1 Error = ' + str(\n",
    "        confusionMatrix[0,1] / (confusionMatrix[0,0]+ confusionMatrix[0,1])))\n",
    "          \n",
    "    print('Type 2 Error = ' + str(\n",
    "        confusionMatrix[1,0] / (confusionMatrix[1,1]+ confusionMatrix[1,0])))\n",
    "    \n",
    "    print('Sensitivity = ' + str(\n",
    "        confusionMatrix[1,1] / (confusionMatrix[1,1]+ confusionMatrix[0,1])))\n",
    "        \n",
    "    print('Specificity = ' + str(\n",
    "        confusionMatrix[0,0] / (confusionMatrix[1,0]+ confusionMatrix[0,0])))\n",
    "          \n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusionMatrix, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Long Covid Status')\n",
    "    plt.ylabel('True Long Covid Status')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predVars = list(trainDf.columns)\n",
    "for a in ['y_pred', 'person_id', 'long_covid']:\n",
    "    predVars.remove(a)\n",
    "target = 'long_covid'\n",
    "#predVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveyPredVars = predVars +[]\n",
    "genePredVars = predVars+[]\n",
    "\n",
    "for a in list(geneAppendDf.columns):\n",
    "    try:\n",
    "        surveyPredVars.remove(a)\n",
    "    except:\n",
    "        #do nothing\n",
    "        notImportant=1\n",
    "        \n",
    "for a in list(surveyAppendDf.columns):\n",
    "    try:\n",
    "        genePredVars.remove(a)\n",
    "    except:\n",
    "        #do nothing\n",
    "        notImportant=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EHR based model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roundedList = []\n",
    "for entry in goldStandardDf['y_pred']:\n",
    "    if entry >=1:\n",
    "        a = 1\n",
    "    else:\n",
    "        a = entry\n",
    "    roundedList = roundedList +[int(round(a,0))]\n",
    "    \n",
    "#tHIS FUNCTION IS MESSING UP A LOT, WE'LL GO BACK AND CHECK ON IT LATER\n",
    "generateConfusionMatrix(goldStandardDf, roundedList, 'EHR Data Classifier Model performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Rergression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "trainModel(trainDf, testDf, predVars,target, linear_model.LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predValues = generatePredValues(trainDf, testDf, linear_model.LinearRegression(), genePredVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Confusion Matrix of Linear Model Genetic Data Classifier'\n",
    "generateConfusionMatrix(testDf, predValues[0], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's include a Confusion Matrix with the weighted average\n",
    "predValues = weightedAverage(trainDf, testDf, \n",
    "                                linear_model.LinearRegression(),\n",
    "                                genePredVars, 0.8)\n",
    "title = 'Confusion Matrix of Genetic and EHR Data Classifier'\n",
    "generateConfusionMatrix(testDf, predValues[0], title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "trainModel(trainDf, testDf, predVars,target, RandomForestRegressor(n_estimators=50, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try giving it more iterations to work with\n",
    "\n",
    "trainModel(trainDf, testDf, predVars,target, RandomForestRegressor(n_estimators=1000, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't seem to be getting any better on our testing data, let's restrict tree complexity for overfitting\n",
    "\n",
    "trainModel(trainDf, testDf, predVars,target, RandomForestRegressor(n_estimators=500, max_depth = 6, min_samples_split = 10, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More general, but not necisarrily better. Let's take a look at that confusion matrix\n",
    "predValues = generatePredValues(trainDf, testDf, \n",
    "                                RandomForestRegressor(n_estimators=500, random_state=0),\n",
    "                                genePredVars)\n",
    "title = 'Confusion Matrix of RF Model Genetic Data Classifier'\n",
    "generateConfusionMatrix(testDf, predValues[0], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's include a Confusion Matrix with the weighted average\n",
    "predValues = weightedAverage(trainDf, testDf, \n",
    "                                RandomForestRegressor(n_estimators=500, random_state=0),\n",
    "                                surveyPredVars, 0.8)\n",
    "title = 'Confusion Matrix of Genetic and EHR Data Classifier'\n",
    "generateConfusionMatrix(testDf, predValues[0], title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "trainModel(trainDf, testDf, predVars,target, MLPRegressor(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Great on training, not good on testing. Let's limit iterations\n",
    "\n",
    "trainModel(trainDf, testDf, predVars,target, MLPRegressor(random_state=0, n_iter_no_change = 50, \n",
    "                                              hidden_layer_sizes = (550,)\n",
    "                                             ,max_iter = 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's getting better with out training data, but worse with out testing\n",
    "\n",
    "#More general, but not necisarrily better. Let's take a look at that confusion matrix\n",
    "predValues = generatePredValues(trainDf, testDf, \n",
    "                                 MLPRegressor(random_state=0, n_iter_no_change = 50, \n",
    "                                              hidden_layer_sizes = (550,)\n",
    "                                             ,max_iter = 500),\n",
    "                                genePredVars)\n",
    "title = 'Confusion Matrix of NN Model Genetic Data Classifier'\n",
    "generateConfusionMatrix(testDf, predValues[0], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's include a Confusion Matrix with the weighted average\n",
    "predValues = weightedAverage(trainDf, testDf, \n",
    "                                MLPRegressor(random_state=0, n_iter_no_change = 3, hidden_layer_sizes = (500,)),\n",
    "                                surveyPredVars, 0.67)\n",
    "title = 'Confusion Matrix of Genetic and EHR Data Classifier'\n",
    "generateConfusionMatrix(testDf, predValues[0], title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "trainModel(trainDf, testDf, predVars,target, KNeighborsRegressor(n_neighbors=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=3 seems to be the best\n",
    "\n",
    "#More general, but not necisarrily better. Let's take a look at that confusion matrix\n",
    "genePredValues = generatePredValues(trainDf, testDf, \n",
    "                                 KNeighborsRegressor(n_neighbors=3),\n",
    "                                genePredVars)\n",
    "title = 'Confusion Matrix of KNN Model Genetic Data Classifier'\n",
    "generateConfusionMatrix(testDf, genePredValues[0], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's include a Confusion Matrix with the weighted average\n",
    "predValues = weightedAverage(trainDf, testDf, \n",
    "                                KNeighborsRegressor(n_neighbors=3),\n",
    "                                genePredVars, 0.73)\n",
    "title = 'Confusion Matrix of Survey and EHR Data Classifier'\n",
    "generateConfusionMatrix(testDf, predValues[0], title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model (survey + mobile device data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#light gbm model needs a specific test and training set, created here\n",
    "personList = list(outerJoinSurveyMobileDevice['person_id'])\n",
    "\n",
    "for a in list(outerJoinSurveyMobileDevice['person_id']):\n",
    "    if a in list(XGBtestSet['person_id']):\n",
    "        personList.remove(a)\n",
    "\n",
    "\n",
    "NonGoldStandard = outerJoinSurveyMobileDevice[outerJoinSurveyMobileDevice['person_id'].isin(personList)]\n",
    "NonGoldStandard = NonGoldStandard.reset_index(drop=True)\n",
    "NonGoldStandard = simplifyingAssumption(NonGoldStandard, 0.93)\n",
    "withNaTrain, withNaTest = makeTestAndTrainSets(NonGoldStandard, 0.7)\n",
    "\n",
    "print(len(personList))\n",
    "#Reset these using XGBtestSet as the gold standard test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NonGoldStandard.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withNaTrain=withNaTrain.drop('person_id', axis=1).reset_index(drop=True)\n",
    "withNaTest=withNaTest.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "yTrain = withNaTrain['long_covid']\n",
    "yTest = withNaTest['long_covid']\n",
    "\n",
    "print(len(yTrain))\n",
    "print(len(yTest))\n",
    "\n",
    "xTrain = withNaTrain.loc[:, withNaTrain.columns !='long_covid']\n",
    "xTest = withNaTest.loc[:, withNaTest.columns != 'long_covid']\n",
    "\n",
    "xTrain = xTrain.loc[:, xTrain.columns !='y_pred']\n",
    "xTest = xTest.loc[:, xTest.columns != 'y_pred']\n",
    "xTest = xTest.loc[:, xTest.columns != 'person_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(xTrain, label = yTrain)\n",
    "test = xgb.DMatrix(xTest, label = yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_dpeth': 5,\n",
    "        'eta':0.6,\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class':3}\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgb.train(param, train, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predXGB = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(yTest, predXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create confusion Matix\n",
    "generateConfusionMatrix(withNaTest, predXGB, 'XGBoost Model Using Mobile Device and Survey Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Control Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainControlDf=trainControlDf.drop('min_covid_dt', axis=1).reset_index(drop=True)\n",
    "testControlDf=testControlDf.drop('min_covid_dt', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "yTrain = trainControlDf['long_covid']\n",
    "yTest = testControlDf['long_covid']\n",
    "\n",
    "print(len(yTrain))\n",
    "print(len(yTest))\n",
    "\n",
    "xTrain = trainControlDf.loc[:, trainControlDf.columns !='long_covid']\n",
    "xTest = testControlDf.loc[:, testControlDf.columns != 'long_covid']\n",
    "\n",
    "xTrain = xTrain.loc[:, xTrain.columns !='y_pred']\n",
    "xTest = xTest.loc[:, xTest.columns != 'y_pred']\n",
    "#xTest = xTest.loc[:, xTest.columns != 'person_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainControl = xgb.DMatrix(xTrain, label = yTrain)\n",
    "testControl = xgb.DMatrix(xTest, label = yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_dpeth': 5,\n",
    "        'eta':0.6,\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class':3}\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlModel=xgb.train(param, trainControl, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predXGBControl = controlModel.predict(testControl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(yTest, predXGBControl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create confusion Matix\n",
    "generateConfusionMatrix(testControlDf, predXGBControl, 'XGBoost Model Using Mobile Device and Survey Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gold Standard Cohort to combine survey data and EHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below creates it's own gold standard test set\n",
    "#We created what can function as a gold standard test set using XGBtestSet, so let's use that instead\n",
    "#alter the code below....\n",
    "withNaTest = XGBtestSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up that test set\n",
    "withNaTest = withNaTest[withNaTest['person_id'].isin(goldStandardDf['person_id'])]\n",
    "withNaTest = withNaTest.sort_values('person_id')\n",
    "withNaTest = withNaTest.reset_index(drop=True)\n",
    "\n",
    "goldStandardDf = goldStandardDf.sort_values('person_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withNaTest = withNaTest.drop('person_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTest = withNaTest['long_covid']\n",
    "print(len(yTest))\n",
    "xTest = withNaTest.loc[:, withNaTest.columns != 'long_covid']\n",
    "xTest = xTest.loc[:, xTest.columns != 'y_pred']\n",
    "test = xgb.DMatrix(xTest, label = yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to combine this model with genetic and EHR data models\n",
    "#use some of the weighted trinary function to get the pred values\n",
    "# Define NAtestDf = with a data frame that includes genetic and survey and mobile device data\n",
    "\n",
    "#The following SHOULD return the same results under these parameters as the genetic and EHR data based model\n",
    "#\n",
    "genePredictorVariables = genePredVars\n",
    "\n",
    "XGBTestDf = test\n",
    "PandasTestDf = goldStandardDf.reset_index(drop=True)\n",
    "p = 0.\n",
    "q = 0.33\n",
    "\n",
    "x = 1-(p+q)\n",
    "\n",
    "regressorXGB= xgb.train(param, train, epochs)\n",
    "\n",
    "    \n",
    "regressorGene = RandomForestRegressor(n_estimators=500, random_state=0)\n",
    "regressorGene.fit(trainDf.loc[:,genePredictorVariables], trainDf['long_covid'])\n",
    "    \n",
    "averageValues = []\n",
    "genePredictions = regressorGene.predict(PandasTestDf.loc[:,genePredictorVariables])\n",
    "surveyPredictions =  regressorXGB.predict(XGBTestDf)\n",
    "#print(len(testDf))\n",
    "    \n",
    "for i in range(0,len(PandasTestDf)):\n",
    "    genePred = genePredictions[i]\n",
    "    surveyPred = surveyPredictions[i]\n",
    "    weightedAverageXGB = (p*(genePred)+q*(surveyPred)+x*(PandasTestDf.loc[i,'y_pred']))\n",
    "    #roundNum = round(weightedAverage,0)\n",
    "    roundNum = weightedAverageXGB\n",
    "    if float(roundNum) <= 0:\n",
    "        roundNum = 0\n",
    "    if float(roundNum) >=1:\n",
    "        roundNum = 1\n",
    "    averageValues = averageValues +[roundNum]\n",
    "    \n",
    "predValuesXGBNotRounded=averageValues\n",
    "\n",
    "predValuesXGB = []\n",
    "for a in predValuesXGBNotRounded:\n",
    "    predValuesXGB = predValuesXGB + [round(a,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateConfusionMatrix(withNaTest, predValuesXGB, 'All 3 models using Gold Standard Cohort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gold Standard Cohort to mix survey, Mobile Device, EHR, and genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to combine this model with genetic and EHR data models\n",
    "#use some of the weighted trinary function to get the pred values\n",
    "# Define NAtestDf = with a data frame that includes genetic and survey and mobile device data\n",
    "\n",
    "#The following SHOULD return the same results under these parameters as the genetic and EHR data based model\n",
    "#\n",
    "genePredictorVariables = genePredVars\n",
    "\n",
    "XGBTestDf = test\n",
    "PandasTestDf = goldStandardDf.reset_index(drop=True)\n",
    "#p = 0.1\n",
    "#q = 0.4\n",
    "p = 0\n",
    "q = 0\n",
    "\n",
    "\n",
    "x = 1-(p+q)\n",
    "\n",
    "regressorXGB= xgb.train(param, train, epochs)\n",
    "\n",
    "    \n",
    "regressorGene =  MLPRegressor(random_state=0, n_iter_no_change = 50, \n",
    "                                              hidden_layer_sizes = (550,)\n",
    "                                             ,max_iter = 500)\n",
    "regressorGene.fit(trainDf.loc[:,genePredictorVariables], trainDf['long_covid'])\n",
    "    \n",
    "averageValues = []\n",
    "genePredictions = regressorGene.predict(PandasTestDf.loc[:,genePredictorVariables])\n",
    "surveyPredictions =  regressorXGB.predict(XGBTestDf)\n",
    "#print(len(testDf))\n",
    "    \n",
    "for i in range(0,len(PandasTestDf)):\n",
    "    genePred = genePredictions[i]\n",
    "    surveyPred = surveyPredictions[i]\n",
    "    weightedAverageXGB = (p*(genePred)+q*(surveyPred)+x*(PandasTestDf.loc[i,'y_pred']))\n",
    "    #roundNum = round(weightedAverage,0)\n",
    "    roundNum = weightedAverageXGB\n",
    "    if float(roundNum) <= 0:\n",
    "        roundNum = 0\n",
    "    if float(roundNum) >=1:\n",
    "        roundNum = 1\n",
    "    averageValues = averageValues +[roundNum]\n",
    "    \n",
    "predValuesXGBNotRounded=averageValues\n",
    "\n",
    "predValuesXGB = []\n",
    "for a in predValuesXGBNotRounded:\n",
    "    predValuesXGB = predValuesXGB + [round(a,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateConfusionMatrix(withNaTest, predValuesXGB, 'All 3 models using Gold Standard Cohort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add them all together to find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDataPredValues = triWeightedAverage(trainDf, goldStandardDf, \n",
    "                    MLPRegressor(random_state=0, n_iter_no_change = 3, hidden_layer_sizes = (500,)),\n",
    "                   MLPRegressor(random_state=0),\n",
    "                   genePredVars, surveyPredVars,\n",
    "                   .17,.16)\n",
    "\n",
    "title = 'Confusion Matrix of Independant Working Models Classifier'\n",
    "generateConfusionMatrix(goldStandardDf, allDataPredValues[0], title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine optimal Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAUROCScore(predValues, testy):\n",
    "    \n",
    "    lr_probs = predValues\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #lr_probs = lr_probs[:, 1]\n",
    "    \n",
    "    # calculate scores\n",
    "    lr_auc = roc_auc_score(testy, lr_probs)\n",
    "    \n",
    "    #print summarized score\n",
    "    #print(': ROC AUC=%.3f' % (lr_auc))\n",
    "    \n",
    "    return lr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that given weights, returns an AUROC score\n",
    "def generateWeightedAUROCScore(p, q, survey_model, gene_model, survey_testing_df, gene_testing_df):\n",
    "    genePredictorVariables = genePredVars\n",
    "\n",
    "    XGBTestDf = survey_testing_df\n",
    "    PandasTestDf = gene_testing_df.reset_index(drop=True)\n",
    "\n",
    "    x = 1-(p+q)\n",
    "\n",
    "    regressorXGB= survey_model\n",
    "    regressorGene =  gene_model\n",
    "    \n",
    "    averageValues = []\n",
    "    genePredictions = regressorGene.predict(gene_testing_df.loc[:,genePredictorVariables])\n",
    "    surveyPredictions =  regressorXGB.predict(survey_testing_df)\n",
    "    #print(len(testDf))\n",
    "\n",
    "    for i in range(0,len(PandasTestDf)):\n",
    "        genePred = genePredictions[i]\n",
    "        surveyPred = surveyPredictions[i]\n",
    "        weightedAverageXGB = (p*(genePred)+q*(surveyPred)+x*(PandasTestDf.loc[i,'y_pred']))\n",
    "        roundNum = weightedAverageXGB\n",
    "        if float(roundNum) <= 0:\n",
    "            roundNum = 0\n",
    "        if float(roundNum) >=1:\n",
    "            roundNum = 1\n",
    "        averageValues = averageValues +[roundNum]\n",
    "\n",
    "    predValuesXGBNotRounded=averageValues\n",
    "    \n",
    "    #Use predValues to get AUROC Score\n",
    "\n",
    "    testy = PandasTestDf['long_covid']\n",
    "    auroc_score = getAUROCScore (predValuesXGBNotRounded, testy)\n",
    "    return auroc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists with posible weights\n",
    "p_weight_list = [num / 100 for num in range(0, 50, 10)]\n",
    "q_weight_list = p_weight_list\n",
    "weight_len = len(p_weight_list) * len(q_weight_list)\n",
    "\n",
    "#Now create a dataframe full of zero's that we can fill in\n",
    "weight_df = pd.DataFrame(np.zeros((weight_len, 3)), columns = ['p', 'q', 'AUROC_Score'])\n",
    "\n",
    "counter = 0\n",
    "for p in p_weight_list:\n",
    "    for q in q_weight_list:\n",
    "        weight_df.loc[counter,'p'] = p\n",
    "        weight_df.loc[counter,'q'] = q\n",
    "        weight_df.loc[counter,'AUROC_Score'] = generateWeightedAUROCScore(p, q, regressorXGB, regressorGene, XGBTestDf, PandasTestDf)\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PandasTestDf['y_pred'].unique()\n",
    "#getAUROCScore(PandasTestDf['y_pred'], PandasTestDf['long_covid'])\n",
    "#[num / 100 for num in range(0, 50, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df.sort_values('AUROC_Score', ascending=False)\n",
    "#We see that the best performing are ~ p=0.1 and q=0.3\n",
    "#Now we do it again, but this time we test everything between p=0.0 -> 0.2, and q= 0.2-> 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists with posible weights\n",
    "p_weight_list = [(0.1 - ((num-10) / 100)) for num in range(0, 20, 4)]\n",
    "q_weight_list = [(0.4 - ((num-10) / 100)) for num in range(0, 20, 4)]\n",
    "weight_len = len(p_weight_list) * len(q_weight_list)\n",
    "\n",
    "#Now create a dataframe full of zero's that we can fill in\n",
    "weight_df = pd.DataFrame(np.zeros((weight_len, 3)), columns = ['p', 'q', 'AUROC_Score'])\n",
    "\n",
    "counter = 0\n",
    "for p in p_weight_list:\n",
    "    for q in q_weight_list:\n",
    "        weight_df.loc[counter,'p'] = p\n",
    "        weight_df.loc[counter,'q'] = q\n",
    "        weight_df.loc[counter,'AUROC_Score'] = generateWeightedAUROCScore(p, q, regressorXGB, regressorGene, XGBTestDf, PandasTestDf)\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df.sort_values('AUROC_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do it one more time, but now just to see if it is +- 0.02\n",
    "\n",
    "#Create lists with posible weights\n",
    "p_weight_list = [(0.12 - ((num-7) / 100)) for num in range(0, 15, 1)]\n",
    "q_weight_list = [(0.34 - ((num-7) / 100)) for num in range(0, 15, 1)]\n",
    "weight_len = len(p_weight_list) * len(q_weight_list)\n",
    "\n",
    "#Now create a dataframe full of zero's that we can fill in\n",
    "weight_df = pd.DataFrame(np.zeros((weight_len, 3)), columns = ['p', 'q', 'AUROC_Score'])\n",
    "\n",
    "counter = 0\n",
    "for p in p_weight_list:\n",
    "    for q in q_weight_list:\n",
    "        weight_df.loc[counter,'p'] = p\n",
    "        weight_df.loc[counter,'q'] = q\n",
    "        weight_df.loc[counter,'AUROC_Score'] = generateWeightedAUROCScore(p, q, regressorXGB, regressorGene, XGBTestDf, PandasTestDf)\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df.sort_values('AUROC_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization: AUROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We probably need to give the function un-rounded predValues for it to work right\n",
    "\n",
    "\n",
    "# roc curve and auc\n",
    "\n",
    "\n",
    "def createAUROC (predValues, testy, modelLabel):\n",
    "    \n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(testy))]\n",
    "    \n",
    "    lr_probs = predValues\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #lr_probs = lr_probs[:, 1]\n",
    "    \n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(testy, ns_probs)\n",
    "    lr_auc = roc_auc_score(testy, lr_probs)\n",
    "    \n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print(modelLabel+': ROC AUC=%.3f' % (lr_auc))\n",
    "    \n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "    \n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label=modelLabel)\n",
    "    \n",
    "    # axis labels\n",
    "    pyplot.xlabel('1 - Specificity')\n",
    "    pyplot.ylabel('Sensitivity')\n",
    "    \n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    \n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    return lr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 3 models with gold standard after introduction of mobile device data\n",
    "#This is weighed the same as the last model, but is returning worse results. Find out why\n",
    "#0.678 (p = 0.1, q = 0.4)\n",
    "createAUROC(predValuesXGBNotRounded, withNaTest['long_covid'], 'All models using Gold Standard Cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 3 models without mobile device data\n",
    "a = createAUROC(allDataPredValues[1], goldStandardDf['long_covid'], 'EHR, Genetic, and Survey')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Survey and EHR data only (KNN)\n",
    "\n",
    "surveyEHRPredValues = weightedAverage(trainDf, goldStandardDf, MLPRegressor(random_state=0), surveyPredVars, 0.67)\n",
    "\n",
    "title = 'Survey and EHR Data Classifier'\n",
    "#generateConfusionMatrix(testDf, predValues[0], title)\n",
    "\n",
    "createAUROC(surveyEHRPredValues[1], goldStandardDf['long_covid'], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genetic and EHR data Only (RF)\n",
    "geneEHRPredValues = weightedAverage(trainDf, goldStandardDf, \n",
    "                                KNeighborsRegressor(n_neighbors=3),\n",
    "                                genePredVars, 0.79)\n",
    "\n",
    "title = 'Genetic and EHR Data Classifier'\n",
    "#generateConfusionMatrix(testDf, predValues[0], title)\n",
    "\n",
    "createAUROC(geneEHRPredValues[1], goldStandardDf['long_covid'], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EHR data only\n",
    "title = 'EHR Data Classifier'\n",
    "createAUROC(goldStandardDf['y_pred'], goldStandardDf['long_covid'], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Survey Data Only\n",
    "surveyPredValues = weightedAverage(trainDf, goldStandardDf, MLPRegressor(random_state=0), surveyPredVars, 0)\n",
    "\n",
    "title = 'Survey Data Classifier'\n",
    "createAUROC(surveyPredValues[1], goldStandardDf['long_covid'], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genetic Data Only\n",
    "genePredValues = weightedAverage(trainDf, goldStandardDf, \n",
    "                                 MLPRegressor(random_state=0, n_iter_no_change = 50, \n",
    "                                              hidden_layer_sizes = (550,)\n",
    "                                             ,max_iter = 500),\n",
    "                                genePredVars, 0)\n",
    "\n",
    "title = 'Genetic Data Classifier'\n",
    "createAUROC(genePredValues[1], goldStandardDf['long_covid'], title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control Model\n",
    "\n",
    "controlPreds =  controlModel.predict(testControl)\n",
    "\n",
    "\n",
    "createAUROC(controlPreds, testControlDf['long_covid'], 'Control Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance And Conglomerate Models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAUROCValues (predValues, testy, modelLabel):\n",
    "    \n",
    "    lr_probs = predValues\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #lr_probs = lr_probs[:, 1]\n",
    "    \n",
    "    listName = []\n",
    "    # calculate scores\n",
    "    lr_auc = roc_auc_score(testy, lr_probs)\n",
    "    \n",
    "    # summarize scores\n",
    "    print(modelLabel+': ROC AUC=%.3f' % (lr_auc))\n",
    "    \n",
    "    # calculate roc curves\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "    returnList = roc_curve(testy, lr_probs)\n",
    "    return returnList\n",
    "\n",
    "\n",
    "    ## plot the roc curve for the model\n",
    "    #pyplot.plot(lr_fpr, lr_tpr, marker='.', label=modelLabel, color = graphColor)\n",
    "    \n",
    "    ## axis labels\n",
    "    #pyplot.xlabel('1 - Specificity')\n",
    "    #pyplot.ylabel('Sensitivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a bunch of plots then show them all\n",
    "\n",
    "#Single Data Sources\n",
    "fpr1, tpr1, _ = getAUROCValues(goldStandardDf['y_pred'], goldStandardDf['long_covid'], 'EHR Data Classifier' )\n",
    "pyplot.plot(fpr1, tpr1, marker='.', label='EHR Data Classifier', color = 'blue')\n",
    "\n",
    "fpr1, tpr1, _ = getAUROCValues(genePredValues[1], goldStandardDf['long_covid'], 'Genetic Data Classifier' )\n",
    "pyplot.plot(fpr1, tpr1, marker='.', label='Genetic Data Classifier', color = 'yellow')\n",
    "\n",
    "fpr1, tpr1, _ = getAUROCValues(surveyPredValues[1], goldStandardDf['long_covid'], 'Survey Data Classifier' )\n",
    "pyplot.plot(fpr1, tpr1, marker='.', label='Survey Data Classifier', color = 'red')\n",
    "\n",
    "#Paired Data Sources\n",
    "fpr2, tpr2, _ = getAUROCValues(geneEHRPredValues[1], goldStandardDf['long_covid'], 'Genetic and EHR Data Classifier')\n",
    "pyplot.plot(fpr2, tpr2, marker='.', label='Genetic and EHR Data Classifier', color = 'green')\n",
    "\n",
    "fpr3, tpr3, _ = getAUROCValues(surveyEHRPredValues[1], goldStandardDf['long_covid'], 'Survey and EHR Data Classifier')\n",
    "pyplot.plot(fpr3, tpr3, marker='.', label='Combined Data Classifier', color = 'purple')\n",
    "\n",
    "#All Data Sources\n",
    "#fpr4, tpr4, _ = getAUROCValues(allDataPredValues[1], goldStandardDf['long_covid'], 'Survey, Genetic, and EHR Data Classifier')\n",
    "#pyplot.plot(fpr4, tpr4, marker='.', label='Survey, Genetic, and EHR Data Classifier', color = 'brown')\n",
    "\n",
    "fpr5, tpr5, _ = getAUROCValues(predValuesXGBNotRounded, withNaTest['long_covid'], 'Mobile Device, Survey, Genetic, and EHR Data Classifier')\n",
    "pyplot.plot(fpr5, tpr5, marker='.', label='Mobile Device, Survey, Genetic, and EHR Data Classifier', color = 'black')\n",
    "\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('1 - Specificity')\n",
    "pyplot.ylabel('Sensitivity')\n",
    "\n",
    "# show the legend\n",
    "#pyplot.legend()\n",
    "    \n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If it can't find it, run \"pip install shap\" in terminal\n",
    "import shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to test a model with shap values\n",
    "def applyShap(model, trainDf, testDf, predVars, targetVar):\n",
    "    \n",
    "    model.fit(trainDf[predVars], trainDf[targetVar])\n",
    "    predictionShap = model.predict(testDf[predVars])\n",
    "\n",
    "    explainer = shap.Explainer(model.predict, testDf[predVars])\n",
    "    shap_values = explainer(testDf[predVars])\n",
    "\n",
    "    \n",
    "    shap.plots.beeswarm(shap_values)\n",
    "    \n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SHAP for feature importance on each model\n",
    "#Start with the Survey Data NN\n",
    "\n",
    "#surveyModelShap = applyShap(MLPRegressor(random_state=0), trainDf, testDf, surveyPredVars, 'long_covid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.waterfall(surveyModelShap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geneModelShap = applyShap( MLPRegressor(random_state=0, n_iter_no_change = 3, hidden_layer_sizes = (500,)), trainDf, testDf, genePredVars, 'long_covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.waterfall(geneModelShap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find a way to do shap for XGBoost algorithm\n",
    "model=xgb.train(param, train, epochs)\n",
    "\n",
    "\n",
    "\n",
    "pred = model.predict(train, output_margin=True)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(train)\n",
    "\n",
    "\n",
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgboost.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venn Diagram with data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EHRPersonIDs = list(LongCovidEHRDf['person_id'])+list(OuterMobileData['person_id'])+list(SurveyDataDF['person_id'])+list(GeneticDataDf['person_id'])\n",
    "\n",
    "#DataDict = {'Patients with EHR Data':set(EHRPersonIDs), 'Patients with Mobile Device Data':set(OuterMobileData['person_id']), \n",
    "            'Patients with Survey Data':set(SurveyDataDF['person_id']), 'Patients with Genetic Data':set(GeneticDataDf['person_id'])}\n",
    "\n",
    "#from venn import venn\n",
    "#venn(DataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make a pretty Venn Diagram, we can export these sets and read them in R using nVennR\n",
    "\n",
    "MobileDevicePersons = OuterMobileData['person_id']\n",
    "SurveyDataPersons = SurveyDataDF['person_id']\n",
    "GeneticDataPersons = GeneticDataDf['person_id']\n",
    "\n",
    "#popList = [MobileDevicePersons, SurveyDataPersons, GeneticDataPersons]\n",
    "\n",
    "storeInBucket(MobileDevicePersons, \"MobileDevicePersons.csv\")\n",
    "storeInBucket(SurveyDataPersons, \"SurveyDataPersons.csv\")\n",
    "storeInBucket(GeneticDataPersons, \"GeneticDataPersons\")\n",
    "storeInBucket(GoldStandardControl, \"GoldStandardAllData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "294.965px",
    "width": "334.468px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
